[
  {
    "objectID": "contents/8/08-.html#tibble",
    "href": "contents/8/08-.html#tibble",
    "title": "8  R for Data Sciences",
    "section": "8.1 tibble",
    "text": "8.1 tibble\ntidyverse mainly deals with tibble instead of data.frame. Therefore this is where we start.\ntibble is a data.frame with different attributes and requirements. The package tibble provides support for tibble. It is included in tidyverse. To load it, you just use the code:\n\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n#> ✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n#> ✔ tibble  3.1.7     ✔ dplyr   1.0.9\n#> ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n#> ✔ readr   2.1.2     ✔ forcats 0.5.1\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n\n\n8.1.1 Create tibbles\nHere is an example of creating tibbles.\n\nExample 8.1  \n\ntbl <- tibble(x=1:5, y=1, z=x^2+y)\ntbl\n#> # A tibble: 5 × 3\n#>       x     y     z\n#>   <int> <dbl> <dbl>\n#> 1     1     1     2\n#> 2     2     1     5\n#> 3     3     1    10\n#> 4     4     1    17\n#> 5     5     1    26\nattributes(tbl)\n#> $class\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n#> \n#> $row.names\n#> [1] 1 2 3 4 5\n#> \n#> $names\n#> [1] \"x\" \"y\" \"z\"\n\nNote that it is more flexible to create a tibble since tibble() will automatically recycle inputs and allows you to refer to variables that you just created.\n\n\n\n\n\n\n\nNote\n\n\n\nIn the past (for a very long time), when using data.frame() to create a data.frame, it will automatically convert strings to factors. This is changed recently that the default setting is not to convert.\nWhen using tibble() to create a tibble, the type of the inputs will never be changed.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn tibble you may use nonsyntactic names as column names, which are invalid R variable names. To refer to these variables, you need to surround them with backticks `.\n\ntb <- tibble(\n    `:)` = \"smile\",\n    ` ` = \"space\",\n    `2000` = \"number\"\n)\ntb\n#> # A tibble: 1 × 3\n#>   `:)`  ` `   `2000`\n#>   <chr> <chr> <chr> \n#> 1 smile space number\n\n\n\n\n\n8.1.2 Differences between tibble and data.frame.\n\n8.1.2.1 Printing\nTibbles have a refined print method that shows only the first 10 rows and all the columns that fit on screen.\n\ndeck <- tibble(suit=rep(c('spades', 'hearts', 'clubs', 'diamonds'), 13), face=rep(1:13, 4))\ndeck\n#> # A tibble: 52 × 2\n#>    suit      face\n#>    <chr>    <int>\n#>  1 spades       1\n#>  2 hearts       2\n#>  3 clubs        3\n#>  4 diamonds     4\n#>  5 spades       5\n#>  6 hearts       6\n#>  7 clubs        7\n#>  8 diamonds     8\n#>  9 spades       9\n#> 10 hearts      10\n#> # … with 42 more rows\n\n\n\n8.1.2.2 Subsetting\nTo get a single value, [[]] or $ should be used, just like for data.frame. These two are almost the same. The only difference is that [[]] accepts positions, but $ only accepts names.\nTo be used in a pipe, the special placeholder . will be used.\n\ndeck %>% .$face\n#>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13  1  2  3  4  5  6  7  8  9 10 11 12\n#> [26] 13  1  2  3  4  5  6  7  8  9 10 11 12 13  1  2  3  4  5  6  7  8  9 10 11\n#> [51] 12 13\n\nWe will talk about pipes later.\n\n\n\n8.1.3 %>% symbol\n%>% is the pipeline symbol, which is another way to connect several functions. Most functions in tidyverse have the first argument data, and both the input data and the output are tibbles. The syntax here is that data %>% function(arguments) is the same as function(data, arguments). The benefit is that it is easier to have many functions consecutively applied to the data. Please see the following example.\n\ndata %>% function1(arguments1)\n    %>% function2(arguments2)\n    %>% function3(arguments3)\n    %>% function4(arguments4)\n\nfunction4(function3(function2(function1(data, arguments1), arguments2), arguments3), arguments4)\n\ndata2 <- function1(data, arguments1)\ndata3 <- function2(data2, arguments2)\ndata4 <- function3(data3, arguments3)\nfunction4(data4, arguments4)\n\nThe readability of the first one is much better than the second one. Comparing to the third one, we don’t need to create a lot of intermedia temporary variables."
  },
  {
    "objectID": "contents/8/08-.html#tidy-data",
    "href": "contents/8/08-.html#tidy-data",
    "title": "8  R for Data Sciences",
    "section": "8.2 Tidy Data",
    "text": "8.2 Tidy Data\nThe same underlying data can be represented in multiple ways. The following example shows the same data organized in four different ways.\n\nExample 8.2 These tibbles are provided by tidyr. You could directly load it from tidyverse.\n\nlibrary(tidyverse)\ndata(table1, package='tidyr')\ndata(table2, package='tidyr')\ndata(table3, package='tidyr')\ndata(table4a, package='tidyr')\ndata(table4b, package='tidyr')\n\n\ntable1\n\n\ntable1\n#> # A tibble: 6 × 4\n#>   country      year  cases population\n#>   <chr>       <int>  <int>      <int>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3 Brazil       1999  37737  172006362\n#> 4 Brazil       2000  80488  174504898\n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\n\n\ntable2\n\n\ntable2\n#> # A tibble: 12 × 4\n#>    country      year type            count\n#>    <chr>       <int> <chr>           <int>\n#>  1 Afghanistan  1999 cases             745\n#>  2 Afghanistan  1999 population   19987071\n#>  3 Afghanistan  2000 cases            2666\n#>  4 Afghanistan  2000 population   20595360\n#>  5 Brazil       1999 cases           37737\n#>  6 Brazil       1999 population  172006362\n#>  7 Brazil       2000 cases           80488\n#>  8 Brazil       2000 population  174504898\n#>  9 China        1999 cases          212258\n#> 10 China        1999 population 1272915272\n#> 11 China        2000 cases          213766\n#> 12 China        2000 population 1280428583\n\n\ntable3\n\n\ntable3\n#> # A tibble: 6 × 3\n#>   country      year rate             \n#> * <chr>       <int> <chr>            \n#> 1 Afghanistan  1999 745/19987071     \n#> 2 Afghanistan  2000 2666/20595360    \n#> 3 Brazil       1999 37737/172006362  \n#> 4 Brazil       2000 80488/174504898  \n#> 5 China        1999 212258/1272915272\n#> 6 China        2000 213766/1280428583\n\n\nSpread across two tibbles.\n\n\ntable4a\n#> # A tibble: 3 × 3\n#>   country     `1999` `2000`\n#> * <chr>        <int>  <int>\n#> 1 Afghanistan    745   2666\n#> 2 Brazil       37737  80488\n#> 3 China       212258 213766\n\ntable4b\n#> # A tibble: 3 × 3\n#>   country         `1999`     `2000`\n#> * <chr>            <int>      <int>\n#> 1 Afghanistan   19987071   20595360\n#> 2 Brazil       172006362  174504898\n#> 3 China       1272915272 1280428583\n\n\n\nDefinition 8.1 A dataset is tidy if\n\nEach variable have its own column.\nEach observation have its own row.\nEach value have its oven cell.\n\n\nThese three conditions are interrelated because it is impossible to only satisfy two of the three. In pratical, we need to follow the instructions:\n\nPut each dataset in a tibble.\nPut each variable in a column.\n\nTidy data is a consistent way to organize your data in R. The main advantages are:\n\nIt is one consistent way of storing data. In other words, this is a consistent data structure that can be used in many cases.\nTo placing variables in columns allows R’s vectorized nature to shine.\n\nAll packages in the tidyverse are designed to work with tidy data.\n\n8.2.1 Tidying datasets\nMost datasets are untidy:\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n\n8.2.1.1 pivot_longer()\nA common problem is that the column names are not names of variables, but values of a variable. For example, table4a above has columns 1999 and 2000. These two names are actually the values of a variable year. In addition, each row represents two observations, not one.\n\ntable4a\n#> # A tibble: 3 × 3\n#>   country     `1999` `2000`\n#> * <chr>        <int>  <int>\n#> 1 Afghanistan    745   2666\n#> 2 Brazil       37737  80488\n#> 3 China       212258 213766\n\nTo tidy this type of dataset, we need to gather those columns into a new pair of variables. We need three parameters:\n\nThe set of columns that represent values. In this case, those are 1999 and 2000.\nThe name of the variable. In this case, it is year. -The name of the variable whose values are spread over the cells. In this case, it is the number of cases.\n\nThen we apply pivot_longer().\n\npivot_longer(table4a, cols=c(`1999`, `2000`), names_to='year', values_to='cases')\n#> # A tibble: 6 × 3\n#>   country     year   cases\n#>   <chr>       <chr>  <int>\n#> 1 Afghanistan 1999     745\n#> 2 Afghanistan 2000    2666\n#> 3 Brazil      1999   37737\n#> 4 Brazil      2000   80488\n#> 5 China       1999  212258\n#> 6 China       2000  213766\n\nWe may also use the pipe %>% symbol.\n\ntable4a %>% pivot_longer(cols=c(`1999`, `2000`), names_to='year', values_to='cases')\n#> # A tibble: 6 × 3\n#>   country     year   cases\n#>   <chr>       <chr>  <int>\n#> 1 Afghanistan 1999     745\n#> 2 Afghanistan 2000    2666\n#> 3 Brazil      1999   37737\n#> 4 Brazil      2000   80488\n#> 5 China       1999  212258\n#> 6 China       2000  213766\n\nWe can do the similar thing to table4b. Then we could combine the two tibbles together.\n\ntidy4a <- table4a %>% \n    pivot_longer(cols=c(`1999`, `2000`), names_to='year', values_to='cases')\ntidy4b <- table4b %>% \n    pivot_longer(cols=c(`1999`, `2000`), names_to='year', values_to='population')\nleft_join(tidy4a, tidy4b)\n#> Joining, by = c(\"country\", \"year\")\n#> # A tibble: 6 × 4\n#>   country     year   cases population\n#>   <chr>       <chr>  <int>      <int>\n#> 1 Afghanistan 1999     745   19987071\n#> 2 Afghanistan 2000    2666   20595360\n#> 3 Brazil      1999   37737  172006362\n#> 4 Brazil      2000   80488  174504898\n#> 5 China       1999  212258 1272915272\n#> 6 China       2000  213766 1280428583\n\npivot_longer() is an updated approach to gather(), designed to be both simpler to use and to handle more use cases. We recommend you use pivot_longer() for new code; gather() isn’t going away but is no longer under active development.\n\n\n8.2.1.2 pivot_wider()\nAnother issuse is that an observation is scattered across multiple rows. Take table2 as an example. An observation is a country in a year, but each observation is spread across two rows.\n\ntable2\n#> # A tibble: 12 × 4\n#>    country      year type            count\n#>    <chr>       <int> <chr>           <int>\n#>  1 Afghanistan  1999 cases             745\n#>  2 Afghanistan  1999 population   19987071\n#>  3 Afghanistan  2000 cases            2666\n#>  4 Afghanistan  2000 population   20595360\n#>  5 Brazil       1999 cases           37737\n#>  6 Brazil       1999 population  172006362\n#>  7 Brazil       2000 cases           80488\n#>  8 Brazil       2000 population  174504898\n#>  9 China        1999 cases          212258\n#> 10 China        1999 population 1272915272\n#> 11 China        2000 cases          213766\n#> 12 China        2000 population 1280428583\n\nWe could apply pivot_wider() to make it tidy. Here we need two arguments.\n\nThe column that contains variable names. Here, it’s type.\nThe column that contains values forms multiple variables. Here, it’s count.\n\n\npivot_wider(table2, names_from='type', values_from='count')\n#> # A tibble: 6 × 4\n#>   country      year  cases population\n#>   <chr>       <int>  <int>      <int>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3 Brazil       1999  37737  172006362\n#> 4 Brazil       2000  80488  174504898\n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\n\nWe can also use the pipe symbol %>%.\n\ntable2 %>% pivot_wider(names_from='type', values_from='count')\n#> # A tibble: 6 × 4\n#>   country      year  cases population\n#>   <chr>       <int>  <int>      <int>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3 Brazil       1999  37737  172006362\n#> 4 Brazil       2000  80488  174504898\n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\n\npivot_wider() is an updated approach to spread(), designed to be both simpler to use and to handle more use cases. We recommend you use pivot_wider() for new code; spread() isn’t going away but is no longer under active development.\n\n\n8.2.1.3 separate()\nIf we would like to split one columns into multiple columns since there are more than one values in a cell, we could use separate().\n\nseparate(table3, rate, into=c('cases', 'population'))\n#> # A tibble: 6 × 4\n#>   country      year cases  population\n#>   <chr>       <int> <chr>  <chr>     \n#> 1 Afghanistan  1999 745    19987071  \n#> 2 Afghanistan  2000 2666   20595360  \n#> 3 Brazil       1999 37737  172006362 \n#> 4 Brazil       2000 80488  174504898 \n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\n\nWe could also use the pipe symbol %>%.\n\ntable3 %>% separate(rate, into=c('cases', 'population'))\n#> # A tibble: 6 × 4\n#>   country      year cases  population\n#>   <chr>       <int> <chr>  <chr>     \n#> 1 Afghanistan  1999 745    19987071  \n#> 2 Afghanistan  2000 2666   20595360  \n#> 3 Brazil       1999 37737  172006362 \n#> 4 Brazil       2000 80488  174504898 \n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\n\nUsing separate, the first argument is the column to be separated. into is where you store the parsed data. If no arguments are given, separate() will split values wherever it sees a non-alphanumeric character. If you would like to specify a separator, you may use the sep argument.\n\nIf sep is set to be a character, the column will be separated by the character.\nIf sep is set to be a vector of integers, the column will be separated by the positions.\n\n\nseparate(table3, rate, into=c('cases', 'population'), sep='/')\n#> # A tibble: 6 × 4\n#>   country      year cases  population\n#>   <chr>       <int> <chr>  <chr>     \n#> 1 Afghanistan  1999 745    19987071  \n#> 2 Afghanistan  2000 2666   20595360  \n#> 3 Brazil       1999 37737  172006362 \n#> 4 Brazil       2000 80488  174504898 \n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\n\n\nseparate(table3, rate, into=c('cases', 'population'), sep=c(2,5))\n#> # A tibble: 6 × 4\n#>   country      year cases population\n#>   <chr>       <int> <chr> <chr>     \n#> 1 Afghanistan  1999 74    5/1       \n#> 2 Afghanistan  2000 26    66/       \n#> 3 Brazil       1999 37    737       \n#> 4 Brazil       2000 80    488       \n#> 5 China        1999 21    225       \n#> 6 China        2000 21    376\n\nNote that in this example, since into only has two columns, the rest of the data are lost.\nAnother useful argument is convert. After separation, the columns are still character columns. If we set convert=TRUE, the columns will be automatically converted into better types if possible.\n\nseparate(table3, rate, into=c('cases', 'population'), convert=TRUE)\n#> # A tibble: 6 × 4\n#>   country      year  cases population\n#>   <chr>       <int>  <int>      <int>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3 Brazil       1999  37737  172006362\n#> 4 Brazil       2000  80488  174504898\n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\n\n\n\n8.2.1.4 unite()\nunite() is the inverse of separate(). The syntax is straghtforward. The default separator is _.\n\ntable3 %>% unite(new, year, rate, sep='_')\n#> # A tibble: 6 × 2\n#>   country     new                   \n#>   <chr>       <chr>                 \n#> 1 Afghanistan 1999_745/19987071     \n#> 2 Afghanistan 2000_2666/20595360    \n#> 3 Brazil      1999_37737/172006362  \n#> 4 Brazil      2000_80488/174504898  \n#> 5 China       1999_212258/1272915272\n#> 6 China       2000_213766/1280428583"
  },
  {
    "objectID": "contents/8/08-.html#dplyr",
    "href": "contents/8/08-.html#dplyr",
    "title": "8  R for Data Sciences",
    "section": "8.3 dplyr",
    "text": "8.3 dplyr\ndplyr is a package used to manipulate data. Here we will just introduce the most basic functions. We will use nycflights13::flights as the example. This dataset comes from the US Bureau of Transportation Statistics. The document can be found here.\nTo load the dataset, please use the following code.\n\n\n\n\nlibrary(nycflights13)\nflights\n#> # A tibble: 336,776 × 19\n#>     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#>  1  2013     1     1      517            515         2      830            819\n#>  2  2013     1     1      533            529         4      850            830\n#>  3  2013     1     1      542            540         2      923            850\n#>  4  2013     1     1      544            545        -1     1004           1022\n#>  5  2013     1     1      554            600        -6      812            837\n#>  6  2013     1     1      554            558        -4      740            728\n#>  7  2013     1     1      555            600        -5      913            854\n#>  8  2013     1     1      557            600        -3      709            723\n#>  9  2013     1     1      557            600        -3      838            846\n#> 10  2013     1     1      558            600        -2      753            745\n#> # … with 336,766 more rows, and 11 more variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\n\n\n8.3.1 filter()\nfilter() allows you to subset observations based on their values. The first argument is the name of the tibble. The rest are the expressions that filter the data. Please see the following examples.\n\nflights %>% filter(month==1, day==1)\n#> # A tibble: 842 × 19\n#>     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#>  1  2013     1     1      517            515         2      830            819\n#>  2  2013     1     1      533            529         4      850            830\n#>  3  2013     1     1      542            540         2      923            850\n#>  4  2013     1     1      544            545        -1     1004           1022\n#>  5  2013     1     1      554            600        -6      812            837\n#>  6  2013     1     1      554            558        -4      740            728\n#>  7  2013     1     1      555            600        -5      913            854\n#>  8  2013     1     1      557            600        -3      709            723\n#>  9  2013     1     1      557            600        -3      838            846\n#> 10  2013     1     1      558            600        -2      753            745\n#> # … with 832 more rows, and 11 more variables: arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,\n#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\n\n\n\n8.3.2 select()\nselect() allows you to filter columns. It is very similar to slicing [].\n\n\n8.3.3 mutate()\nmutate() is used to add new columns that are functions of existing columns.\n\nflights %>% mutate(gain=arr_delay-dep_delay, hours=air_time/60, gain_per_hour=gain/hours)\n#> # A tibble: 336,776 × 22\n#>     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#>  1  2013     1     1      517            515         2      830            819\n#>  2  2013     1     1      533            529         4      850            830\n#>  3  2013     1     1      542            540         2      923            850\n#>  4  2013     1     1      544            545        -1     1004           1022\n#>  5  2013     1     1      554            600        -6      812            837\n#>  6  2013     1     1      554            558        -4      740            728\n#>  7  2013     1     1      555            600        -5      913            854\n#>  8  2013     1     1      557            600        -3      709            723\n#>  9  2013     1     1      557            600        -3      838            846\n#> 10  2013     1     1      558            600        -2      753            745\n#> # … with 336,766 more rows, and 14 more variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n#> #   gain <dbl>, hours <dbl>, gain_per_hour <dbl>\n\nIf you only want to see the new columns, transmute() can be used.\n\nflights %>% transmute(gain=arr_delay-dep_delay, hours=air_time/60, gain_per_hour=gain/hours)\n#> # A tibble: 336,776 × 3\n#>     gain hours gain_per_hour\n#>    <dbl> <dbl>         <dbl>\n#>  1     9 3.78           2.38\n#>  2    16 3.78           4.23\n#>  3    31 2.67          11.6 \n#>  4   -17 3.05          -5.57\n#>  5   -19 1.93          -9.83\n#>  6    16 2.5            6.4 \n#>  7    24 2.63           9.11\n#>  8   -11 0.883        -12.5 \n#>  9    -5 2.33          -2.14\n#> 10    10 2.3            4.35\n#> # … with 336,766 more rows\n\nHere are an (incomplete) list of supported operators and functions.\n\nArithmetic operators: +, -, *, /, ^.\nModular arithmetic: %/% (integer division), %% (remainder).\nLogs: log(), log2(), log10().\nCumulative and rolling aggregates: cumsum(), cumprod(), cummin(), cummax(), cummean()\nLogical comparisons: <, <=, >, >=, !=.\n\n\n\n8.3.4 summarize() and group_by()\nsummarize() collapses a dataset to a single row. It computes values across all rows. It is usually paired with group_by(). Here are some examples.\n\nExample 8.3  \n\nflights %>% group_by(year, month, day) %>% \n    summarize(delay=mean(dep_delay, na.rm=TRUE))\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the\n#> `.groups` argument.\n#> # A tibble: 365 × 4\n#> # Groups:   year, month [12]\n#>     year month   day delay\n#>    <int> <int> <int> <dbl>\n#>  1  2013     1     1 11.5 \n#>  2  2013     1     2 13.9 \n#>  3  2013     1     3 11.0 \n#>  4  2013     1     4  8.95\n#>  5  2013     1     5  5.73\n#>  6  2013     1     6  7.15\n#>  7  2013     1     7  5.42\n#>  8  2013     1     8  2.55\n#>  9  2013     1     9  2.28\n#> 10  2013     1    10  2.84\n#> # … with 355 more rows\n\n\n\nExample 8.4  \n\ndelays <- flights %>% \n    group_by(dest) %>% \n    summarize(\n        count=n(), \n        dist=mean(distance, na.rm=TRUE),\n        delay=mean(arr_delay, na.rm=TRUE)\n    ) %>% \n    filter(count>20, dest!='HNL')\ndelays\n#> # A tibble: 96 × 4\n#>    dest  count  dist delay\n#>    <chr> <int> <dbl> <dbl>\n#>  1 ABQ     254 1826   4.38\n#>  2 ACK     265  199   4.85\n#>  3 ALB     439  143  14.4 \n#>  4 ATL   17215  757. 11.3 \n#>  5 AUS    2439 1514.  6.02\n#>  6 AVL     275  584.  8.00\n#>  7 BDL     443  116   7.05\n#>  8 BGR     375  378   8.03\n#>  9 BHM     297  866. 16.9 \n#> 10 BNA    6333  758. 11.8 \n#> # … with 86 more rows\n\n\ngroup_by() can also be used together with mutate() and filter().\n\nExample 8.5  \n\nflights %>%\n    group_by(dest) %>%\n    filter(n() > 365) %>%\n    filter(arr_delay > 0) %>%\n    mutate(prop_delay = arr_delay / sum(arr_delay)) %>%\n    select(year:day, dest, arr_delay, prop_delay)\n#> # A tibble: 131,106 × 6\n#> # Groups:   dest [77]\n#>     year month   day dest  arr_delay prop_delay\n#>    <int> <int> <int> <chr>     <dbl>      <dbl>\n#>  1  2013     1     1 IAH          11  0.000111 \n#>  2  2013     1     1 IAH          20  0.000201 \n#>  3  2013     1     1 MIA          33  0.000235 \n#>  4  2013     1     1 ORD          12  0.0000424\n#>  5  2013     1     1 FLL          19  0.0000938\n#>  6  2013     1     1 ORD           8  0.0000283\n#>  7  2013     1     1 LAX           7  0.0000344\n#>  8  2013     1     1 DFW          31  0.000282 \n#>  9  2013     1     1 ATL          12  0.0000400\n#> 10  2013     1     1 DTW          16  0.000116 \n#> # … with 131,096 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe already use it in the above examples. This is to compute the number of observations in the current group. This function is implemented specifically for each data source and can only be used from within summarise(), mutate() and filter()."
  },
  {
    "objectID": "contents/8/08-.html#ggplot2",
    "href": "contents/8/08-.html#ggplot2",
    "title": "8  R for Data Sciences",
    "section": "8.4 ggplot2",
    "text": "8.4 ggplot2\nThis is the graphing package for R in tidyverse. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more faster by learning one system and applying it in many places. The main function is ggplot().\nggplot2 will be uploaded with tidyverse.\n\nlibrary(tidyverse)\n\nWe use the dataset mpg as the example. This dataset comes with ggplot2. Once you load ggplot2 you can directly find the dataset by typing mpg.\n\nmpg\n#> # A tibble: 234 × 11\n#>    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n#>    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n#>  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n#>  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n#>  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n#>  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n#>  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n#>  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n#>  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n#>  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n#>  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n#> 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n#> # … with 224 more rows\n\nThe syntax for ggplot() is ::: {.cell}\nggplot(data = <DATA>) +\n    <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n:::\nggplot(data=<DATA>) create a plot without any geometric elements. It simply creates the canvas paired with the dataset.\nThen you add one or more layers to ggplot() to complete the graph. The function <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>)) adds a layer to the plot. You may add as many layers as you want.\nEach geom function takes a mapping argument. This defines how variables in the dataset are mapped to visual properties. mapping is always paired with aes(x=, y=). This\nHere is a quick example.\n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\n\n8.4.1 Aesthetic mappings\nAn aesthetic is a visual property of the objects in your plot. It include things like the size, the shape or the color of the points. The variables set in aes() will change the aesthetic apperance of the geometric objects according to the variables. If the variables are set outside aes(), the apperance will be fixed. Please see the following examples.\nNote that the variables in aes() other than x and y will automatically get legends.\n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy, color = class))\n\n\n\n\n\n\n8.4.2 facet\nFor categorical data, you can split the plot into facets. This facet function will be attached as a layer followed by a + sign.\n\nTo facet your plot by a single variable, use facet_wrap(). The first argument should be a formula, which you create with ~ followed by a variable name.\nTo facet your plot by two variables, use facet_grid(). The first argument is a formula which contains two variable names separated by a ~.\n\n\nExample 8.6  \n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy)) +\n    facet_wrap(~ class, nrow = 2)\n\n\n\n\nYou may look at the variables to see the relations with the plot.\n\nunique(mpg$class)\n#> [1] \"compact\"    \"midsize\"    \"suv\"        \"2seater\"    \"minivan\"   \n#> [6] \"pickup\"     \"subcompact\"\n\n\n\nExample 8.7  \n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy)) +\n    facet_grid(drv ~ cyl)\n\n\n\n\nYou may look at the variables to see the relations with the plot.\n\nunique(mpg$drv)\n#> [1] \"f\" \"4\" \"r\"\nunique(mpg$cyl)\n#> [1] 4 6 8 5\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe ~ symbol is used to define a formula. The formula is a R object, which provide the pattern of a “formula”. Therefore drv~cyl means that drv is a function of cyl.\n\n\n\n\n8.4.3 geom objects\nA geom is the geometrical object that a plot uses to represent data. When drawing plots, you just need to attach those geometrical objects to a ggplot canvas with + symbol. Some of the geometrical objects can automatically do statistical transformations. The statistical transformations is short for stat, and the stat argument in those geom will show which statistical transformations are applied.\n\ngeom_point() draws scatter plot.\ngeom_smooth() draws smooth line approximation.\ngeom_bar() draws bar plot.\ngeom_histogram() draws histogram.\n\nThe arguments can be written in ggplot(). All the later geom will get those arguments from ggplot(). If the arguments are written again in geom object, it will override the ggplot() arguments.\n\nExample 8.8  \n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\nggplot(data = mpg) +\n    geom_smooth(mapping = aes(x = displ, y = hwy), formula = y ~ x, method = \"loess\")\n\n\n\n\nggplot(data = mpg) +\n    geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv), formula = y ~ x, method = \"loess\")\n\n\n\n\nggplot(data = mpg) +\n    geom_point(mapping = aes(x = displ, y = hwy)) +\n    geom_smooth(mapping = aes(x = displ, y = hwy), formula = y ~ x, method = \"loess\")\n\n\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\n    geom_point(mapping = aes(color = class)) +\n    geom_smooth(\n        data = filter(mpg, class == \"subcompact\"),\n        se = FALSE, \n        formula = y ~ x, method = \"loess\"\n        )"
  },
  {
    "objectID": "contents/8/08-.html#exercises",
    "href": "contents/8/08-.html#exercises",
    "title": "8  R for Data Sciences",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\n\nExercise 8.1 How can you tell if an object is a tibble?\n\n\nExercise 8.2 Compare and contrast the following operations on a data.frame and equivalent tibble. What is different? Why might the default data frame behaviors cause you frustration?\n\ndf <- data.frame(abc = 1, xyz = \"a\")\ndf$x\ndf[, \"xyz\"]\ndf[, c(\"abc\", \"xyz\")]\n\n\n\nExercise 8.3 If you have the name of a variable stored in an object, e.g., var <- \"xyz\", how can you extract the reference variable from a tibble? You may use the following codes to get a tibble.\n\ntbl <- tibble(abc = 1, xyz = \"a\")\n\n\n\nExercise 8.4 Practice referring to nonsyntactic names in the following data.frame by:\n\nExtracting the variable called 1.\nCreating a new column called 3, which is 2 divided by 1.\nRenaming the columns to one, two, and three:\n\n\nannoying <- tibble(\n`1` = 1:10,\n`2` = `1` * 2 + rnorm(length(`1`))\n)\n\n\n\nExercise 8.5 Both unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE?\n\n\nExercise 8.6 Use flights dataset. Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight."
  },
  {
    "objectID": "contents/8/08-.html#projects",
    "href": "contents/8/08-.html#projects",
    "title": "8  R for Data Sciences",
    "section": "8.7 Projects",
    "text": "8.7 Projects\n\n\n\n\n\n\n\n\nExercise 8.7 Please make the following data tidy.\n\nlibrary(tidyverse)\ndf <- tibble(Day=1:5, `Plant_A_Height (cm)`=c(0.5, 0.7, 0.9, 1.3, 1.8), `Plant_B_Height (cm)`=c(0.7, 1, 1.5, 1.8, 2.2))\n\n\n\nExercise 8.8 Please use the flights dataset. Please find all flights that :\n\nHad an arrival delay of two or more hours.\nFlew to IAH or HOU.\nWere operated by United, American or Delta.\nDeparted in summer (July, August, and September).\nArrived more than two hours late, but didn’t leave late.\nWere delayed by at least an hour, but made up over 30 minutes in flight.\nDeparted between midnight and 6 a.m. (inclusive).\n\n\n\nExercise 8.9 Re-create the R code necessary to generate the following graphs. The dataset is mpg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] Wickham, H. and Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media."
  },
  {
    "objectID": "contents/2/02-.html#sec-path",
    "href": "contents/2/02-.html#sec-path",
    "title": "2  Python Basics",
    "section": "2.1 Built-in Types: numeric types and str",
    "text": "2.1 Built-in Types: numeric types and str\nThis section is based on [1].\nThere are several built-in data structures in Python. Here is an (incomplete) list:\n\nNone\nBoolean – True, False\nNumeric Types — int, float, complex\nText Sequence Type — str\nSequence Types — list\nMap type - dict\n\nWe will cover numeric types and strings in this section. The rests are either simple that are self-explained, or not simple that will be discussed later.\n\n2.1.1 Numeric types and math expressions\nNumeric types are represented by numbers. If there are no confusions, Python will automatically detect the type.\n\nx = 1 # x is an int.\ny = 2.0 # y is a float.\n\nPython can do math just like other programming languages. The basic math operations are listed as follows.\n\n+, -, *, /, >, <, >=, <= works as normal.\n** is the power operation.\n% is the mod operation.\n!= is not equal\n\n\n\n2.1.2 str\nScalars are represented by numbers and strings are represented by quotes. Example:\n\nx = 1       # x is a scalar.\ny = 's'     # y is a string with one letter.\nz = '0'     # z loos like a number, but it is a string.\nw = \"Hello\" # w is a string with double quotes.\n\nHere are some facts.\n\nFor strings, you can use either single quotes ' or double quotes \".\n\\ is used to denote escaped words. You may find the list Here.\nThere are several types of scalars, like int, float, etc.. Usually Python will automatically determine the type of the data, but sometimes you may still want to declare them manually.\nYou can use int(), str(), etc. to change types.\n\nAlthough str is a built-in type, there are tons of tricks with str, and there are tons of packages related to strings. Generally speaking, to play with strings, we are interested in two types of questions.\n\nPut information together to form a string.\nExtract information from a string. We briefly talk about these two tasks.\n\n\n\n\n\n\n\nNote\n\n\n\nThere is a very subtle relations between the variable / constant and the name of the variable / constant. We will talk about these later.\n\n\n\nExample 2.1 Here is an example of playing with strings. Please play with these codes and try to understand what they do.\n\nimport re\n\ndef clean_strings(strings):\n    result = []\n    for value in strings:\n        value = value.strip()\n        value = re.sub('[!#?]', '', value)\n        value = value.title()\n        result.append(value)\n    return result\n\nstates = [' Alabama ', 'Georgia!', 'Georgia', 'georgia', 'FlOrIda',\n          'south carolina##', 'West virginia?']\nprint(clean_strings(states))\n\n['Alabama', 'Georgia', 'Georgia', 'Georgia', 'Florida', 'South Carolina', 'West Virginia']"
  },
  {
    "objectID": "contents/2/02-.html#fundamentals",
    "href": "contents/2/02-.html#fundamentals",
    "title": "2  Python Basics",
    "section": "2.2 Fundamentals",
    "text": "2.2 Fundamentals\nThis section is mainly based on [2].\n\n2.2.1 Indentation\nOne key feature about Python is that its structures (blocks) is determined by Indentation.\nLet’s compare with other languages. Let’s take C as an example.\n\n/*This is a C function.*/\nint f(int x){return x;}\n\nThe block is defined by {} and lines are separated by ;. space and newline are not important when C runs the code. It is recommended to write codes in a “beautiful, stylish” format for readibility, as follows. However it is not mandatary.\n\n/*This is a C function.*/\nint f(int x) {\n   return x;\n}\n\nIn Python, blocks starts from : and then are determined by indents. Therefore you won’t see a lot of {} in Python, and the “beautiful, stylish” format is mandatary.\n\n# This is a Python function.\ndef f(x):\n    return x\n\nThe default value for indentation is 4 spaces, which can be changed by users. We will just use the default value in this course.\n\n\n\n\n\n\nNote\n\n\n\nIt is usually recommended that one line of code should not be very long. If you do have one, and it cannot be shortened, you may break it into multiline codes directly in Python. However, since indentation is super important in Python, when break one line code into multilines, please make sure that everything is aligned perfectly. Please see the following example.\n\nresults = shotchartdetail.ShotChartDetail(\n            team_id = 0,\n            player_id = 201939,\n            context_measure_simple = 'FGA',\n            season_nullable = '2021-22',\n            season_type_all_star = 'Regular Season')\n\n\n\n\n\n2.2.2 Binary operators and comparisons\nMost binary operators behaves as you expected. Here I just want to mention == and is.\n\n== is testing whehter these two objects have the same value.\nis is testing whether these two objects are exactly the same.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may use id(x) to check the id of the object x. Two objects are identical if they have the same id.\n\n\n\n\n\n2.2.3 import\nIn Python a module is simply a file with the .py extension containing Python code. Assume that we have a Python file example.py stored in the folder assests/codes/. The file is as follows.\n\n# from assests/codes/example.py\n\ndef f(x):\n    print(x)\n\nA = 'You get me!'\n\nYou may get access to this function and this string in the following way.\n\nfrom assests.codes import example\n\nexample.f(example.A)\n\nYou get me!\n\n\n\n\n2.2.4 Comments\nAny text preceded by the hash mark (pound sign) # is ignored by the Python interpreter. In many IDEs you may use hotkeys to directly toggle multilines as comments. For example, in VS Code the default setting for toggling comments is ctrl+/.\n\n\n2.2.5 Dynamic references, strong types\nIn some programming languages, you have to declare the variable’s name and what type of data it will hold. If a variable is declared to be a number, it can never hold a different type of value, like a string. This is called static typing because the type of the variable can never change.\nPython is a dynamically typed language, which means you do not have to declare a variable or what kind of data the variable will hold. You can change the value and type of data at any time. This could be either great or terrible news.\nOn the other side, “dynamic typed” doesn’t mean that types are not important in Python. You still have to make sure that the types of all variables meet the requirements of the operations used.\n\na = 1\nb = 2\nb = '2'\nc = a + b\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\nIn this example, b was first assigned by a number, and then it was reassigned by a str. This is totally fine since Python is dynamically types. However later when adding a and b, the type error occurs since you cannot add a number and a str.\n\n\n\n\n\n\nNote\n\n\n\nYou may always use type(x) to detect the type of the object x.\n\n\n\n\n2.2.6 Everything is an object\nEvery number, string, data structure, function, class, module, and so on exists in the Python interpreter in its own “box”, which is referred to as a Python object.\nEach object has an associated type (e.g., string or function) and internal data. In practice this makes the language very flexible, as even functions can be treated like any other object.\nEach object might have attributes and/or methods attached.\n\n\n2.2.7 Mutable and immutable objects\nAn object whose internal state can be changed is mutable. On the other hand, immutable doesn’t allow any change in the object once it has been created.\nSome objects of built-in type that are mutable are:\n\nLists\nDictionaries\nSets\n\nSome objects of built-in type that are immutable are:\n\nNumbers (Integer, Rational, Float, Decimal, Complex & Booleans)\nStrings\nTuples\n\n\nExample 2.2 (Tuples are not really “immutable”) You can treat a tuple as a container, which contains some objects. The relations between the container and its contents are immutable, but the objects it holds might be mutable. Please check the following example.\n\ncontainer = ([1], [2])\nprint('This is `container`: ', container)\nprint('This is the id of `container`: ', id(container))\nprint('This is the id of the first list of `container`: ', id(container[0]))\n\ncontainer[0].append(2)\nprint('This is the new `container`: ', container)\nprint('This is the id of the new `container`: ', id(container))\nprint('This is the id of the first list (which is updated) of the new `container`: ', id(container[0]))\n\nThis is `container`:  ([1], [2])\nThis is the id of `container`:  1946833634880\nThis is the id of the first list of `container`:  1946833486272\nThis is the new `container`:  ([1, 2], [2])\nThis is the id of the new `container`:  1946833634880\nThis is the id of the first list (which is updated) of the new `container`:  1946833486272\n\n\nYou can see that the tuple container and its first object stay the same, although we add one element to the first object."
  },
  {
    "objectID": "contents/2/02-.html#flows-and-logic",
    "href": "contents/2/02-.html#flows-and-logic",
    "title": "2  Python Basics",
    "section": "2.3 Flows and Logic",
    "text": "2.3 Flows and Logic\n\n2.3.1 for loop\n\nrange(10)\nlist\n\n\n\n2.3.2 if conditional control"
  },
  {
    "objectID": "contents/2/02-.html#list",
    "href": "contents/2/02-.html#list",
    "title": "2  Python Basics",
    "section": "2.4 list",
    "text": "2.4 list\n\n\n\n\n\n\nNote\n\n\n\nIn Python, a list is an ordered sequence of object types and a string is an ordered sequence of characters.\n\n\n\nAccess to the data\nSlicing\nMethods\n\nappend and +\nextend\npop\nremove\n\nin\nfor\nlist()\nsorted\nstr.split\nstr.join\n\n\n2.4.1 List Comprehension\nList Comprehension is a convenient way to create lists based on the values of an existing list. It cannot provide any real improvement to the performance of the codes, but it can make the codes shorter and easier to read.\nThe format of list Comprehension is\nnewlist = [expression for item in iterable if condition == True]"
  },
  {
    "objectID": "contents/2/02-.html#dict",
    "href": "contents/2/02-.html#dict",
    "title": "2  Python Basics",
    "section": "2.5 dict",
    "text": "2.5 dict\n\nAccess to the data\nMethods\n\ndirectly add items\nupdate\nget\nkeys\nvalues\nitems\n\ndict()\ndictionary comprehension"
  },
  {
    "objectID": "contents/2/02-.html#exercises",
    "href": "contents/2/02-.html#exercises",
    "title": "2  Python Basics",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\nMost problems are based on [3], [1] and [4].\n\nExercise 2.1 (Indentation) Please tell the differences between the following codes. If you don’t understand for don’t worry about it. Just focus on the indentation and try to understand how the codes work.\n\nfor i in range(5):\n    print('Hello world!')\nprint('Hello world!')\n\n\nfor i in range(5):\n    print('Hello world!')\n    print('Hello world!')\n\n\nfor i in range(5):\nprint('Hello world!')\nprint('Hello world!')\n\n\nfor i in range(5):\n    pass\nprint('Hello world!')\nprint('Hello world!')\n\n\n\nExercise 2.2 (Play with built-in data types) Please first guess the results of all expressions below, and then run them to check your answers.\n\nprint(True and True)\nprint(True or True)\nprint(False and True)\nprint((1+1>2) or (1-1<1))\n\n\n\nExercise 2.3 (== vs is) Please explain what happens below.\n\na = 1\nb = 1.0\nprint(type(a))\nprint(type(b))\n\nprint(a == b)\nprint(a is b)\n\n<class 'int'>\n<class 'float'>\nTrue\nFalse\n\n\n\n\nExercise 2.4 (Play with strings) Please excute the code below line by line and explain what happens in text cells.\n\n# 1\nanswer = 10\nwronganswer = 11\ntext1 = \"The answer to this question is {}. If you got {}, you are wrong.\".format(answer, wronganswer)\nprint(text1)\n\n# 2\nvar = True\ntext2 = \"This is {}.\".format(var)\nprint(text2)\n\n# 3\nword1 = 'Good '\nword2 = 'buy. '\ntext3 = (word1 + word2) * 3\nprint(text3)\n\n# 4\nsentence = \"This is\\ngood enough\\nfor a exercise to\\nhave so many parts. \" \\\n           \"We would also want to try this symbol: '. \" \\\n           \"Do you know how to type \\\" in double quotes?\"\nprint(sentence)\n\nThe answer to this question is 10. If you got 11, you are wrong.\nThis is True.\nGood buy. Good buy. Good buy. \nThis is\ngood enough\nfor a exercise to\nhave so many parts. We would also want to try this symbol: '. Do you know how to type \" in double quotes?\n\n\n\n\nExercise 2.5 (split and join) Please excute the code below line by line and explain what happens in text cells.\n\nsentence = 'This is an example of a sentence that I expect you to split.'\n\nwordlist = sentence.split(' ')\n\nnewsentence = '\\n'.join(wordlist)\nprint(newsentence)\n\n\n\nExercise 2.6 (List reference) Please finish the following tasks.\n\nGiven the list a, make a new reference b to a. Update the first entry in b to be 0. What happened to the first entry in a? Explain your answer in a text block.\nGiven the list a, make a new copy b of the list a using the function list. Update the first entry in b to be 0. What happened to the first entry in a? Explain your answer in a text block.\n\n\n\nExercise 2.7 (List comprehension) Given a list of numbers, use list comprehension to remove all odd numbers from the list:\n\nnumbers = [3,5,45,97,32,22,10,19,39,43]\n\n\n\nExercise 2.8 (More list comprehension) Use list comprehension to find all of the numbers from 1-1000 that are divisible by 7.\n\n\nExercise 2.9 (More list comprehension) Count the number of spaces in a string.\n\n\nExercise 2.10 (More list comprehension) Use list comprehension to get the index and the value as a tuple for items in the list ['hi', 4, 8.99, 'apple', ('t,b', 'n')]. Result would look like [(index, value), (index, value), ...].\n\n\nExercise 2.11 (More list comprehension) Use list comprehension to find the common numbers in two lists (without using a tuple or set) list_a = [1, 2, 3, 4], list_b = [2, 3, 4, 5].\n\n\nExercise 2.12 (Probability) Compute the probability that two people out of 23 share the same birthday. The math formula for this is \\[1-\\frac{365!/(365-23)!}{365^{23}}=1-\\frac{365}{365}\\cdot\\frac{365-1}{365}\\cdot\\frac{365-2}{365}\\cdot\\ldots\\cdot\\frac{365-22}{365}.\\]\n\nTo directly use the formula we have to use a high performance math package, e.g. math. Please use math.factorial to compute the above formula.\nPlease use the right hand side of the above formula to compute the probability using the following steps.\n\nPlease use the list comprehension to create a list \\(\\left[\\frac{365}{365},\\frac{365-1}{365},\\frac{365-2}{365},\\ldots,\\frac{365-22}{365}\\right]\\).\nUse numpy.prod to compute the product of elements of the above list.\nCompute the probability by finishing the formula.\n\nPlease use time to test which method mentioned above is faster."
  },
  {
    "objectID": "contents/2/02-.html#projects",
    "href": "contents/2/02-.html#projects",
    "title": "2  Python Basics",
    "section": "2.7 Projects",
    "text": "2.7 Projects\nMost projects are based on [2], [5].\n\nExercise 2.13 (Determine the indefinite article) Please finish the following tasks.\n\nPlease construct a list aeiou that contains all vowels.\nGiven a word word, we would like to find the indefinite article article before word. (Hint: the article should be an if the first character of word is a vowel, and a if not.)\n\n\n\n\nClick for Hint.\n\n\nSolution. Consider in, .lower() and if structure.\n\n\n\nExercise 2.14 (Datetime and files names) We would like to write a program to quickly generate N files. Every time we run the code, N files will be generated. We hope to store all files generated and organize them in a neat way. To achieve this, one way is to create a subfolder for each run and store all files generated during that run in the particular subfolder. Since we would like to make it fast, the real point of this task is to find a way to automatically generate the file names for the files generated and the folder names for the subfolders generated. You don’t need to worry about the contents of the files and empty files are totally fine for this problem.\n\n\n\nClick for Hint.\n\n\nSolution. One way to automatically generate file names and folder names is to use the date and the time when the code is run. Please check datetime package for getting and formatting date/time, and os packages for playing with files and folders.\n\n\n\nExercise 2.15 (Color the Gnomic data) We can use ASCII color codes in the string to change the color of strings, as an example \\033[91m for red and \\033[94m for blue. See the following example.\n\nprint('\\033[91m'+'red'+'\\033[92m'+'green'+'\\033[94m'+'blue'+'\\033[93m'+'yellow')\n\nConsider an (incomplete) Gnomic data given below which is represented by a long sequence of A, C, T and G. Please color it using ASCII color codes.\n\nGnomicdata = 'TCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTGGCTGTCACTCGG'\\\n             'CTGCATGCTTAGTGCACTCACGCAGTATAATTAATAACTAATTACTGTCGTTGACAGGAC'\\\n             'ACGAGTAACTCGTCTATCTTCTGCAGGCTGCTTACGGTTTCGTCCGTGTTGCAGCCGATC'\\\n             'ATCAGCACATCTAGGTTTTGTCCGGGTGTGACCGAAAGGTAAGATGGAGAGCCTTGTCCC'\\\n             'TGGTTTCAACGAGAAAACACACGTCCAACTCAGTTTGCCTGTTTTACAGGTTCGCGACGT'\\\n             'GCTCGTACGTGGCTTTGGAGACTCCGTGGAGGAGGTCTTATCAGAGGCACGTCAACATCT'\\\n             'TAAAGATGGCACTTGTGGCTTAGTAGAAGTTGAAAAAGGCGTTTTGCCTCAACTTGAACA'\\\n             'GCCCTATGTGTTCATCAAACGTTCGGATGCTCGAACTGCACCTCATGGTCATGTTATGGT'\\\n             'TGAGCTGGTAGCAGAACTCGAAGGCATTCAGTACGGTCGTAGTGGTGAGACACTTGGTGT'\\\n             'CCTTGTCCCTCATGTGGGCGAAATACCAGTGGCTTACCGCAAGGTTCTTCTTCGTAAGAA'\\\n             'CGGTAATAAAGGAGCTGGTGGCCATAGTTACGGCGCCGATCTAAAGTCATTTGACTTAGG'\\\n             'CGACGAGCTTGGCACTGATCCTTATGAAGATTTTCAAGAAAACTGGAACACTAAACATAG'\n\n\n\n\nClick for Hint.\n\n\nSolution (Hint). You may use if to do the conversion. Or you may use dict to do the conversion.\n\n\n\nExercise 2.16 (sorted) Please read through the Key funtions in this article, and sort the following two lists.\n\nSort list1 = [[11,2,3], [2, 3, 1], [5,-1, 2], [2, 3,-8]] according to the sum of each list.\nSort list2 = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4},{'a': 5, 'b': 2}] according to the b value of each dictionary.\n\n\n\nExercise 2.17 (Fantasy Game Inventory) You are creating a fantasy video game. The data structure to model the player’s inventory will be a dictionary where the keys are string values describing the item in the inventory and the value is an integer value detailing how many of that item the player has. For example, the dictionary value {'rope': 1, 'torch': 6, 'gold coin': 42, 'dagger': 1, 'arrow': 12} means the player has 1 rope, 6 torches, 42 gold coins, and so on.\nWrite a program to take any possible inventory and display it like the following:\n\nInventory:\n12 arrow\n42 gold coin\n1 rope\n6 torch\n1 dagger\nTotal number of items: 62\n\n\n\n\n\n\n[1] Youens-Clark, K. (2020). Tiny python projects. Manning Publications.\n\n\n[2] McKinney, W. (2017). Python for data analysis: Data wrangling with pandas, NumPy, and IPython. O’Reilly Media.\n\n\n[3] Shaw, Z. A. (2017). Learn python 3 the hard way. Addison Wesley.\n\n\n[4] Sweigart, A. (2020). Automate the boring stuff with python, 2nd edition practical programming for total beginners: Practical programming for total beginners. No Starch Press.\n\n\n[5] Klosterman, S. (2021). Data science projects with python: A case study approach to gaining valuable insights from real data with machine learning. Packt Publishing, Limited."
  },
  {
    "objectID": "contents/8/08-.html#an-example",
    "href": "contents/8/08-.html#an-example",
    "title": "8  R for Data Sciences",
    "section": "8.5 An Example",
    "text": "8.5 An Example\nLet us explore the tuberculosis cases data. The dataset is provided by WHO and can be downloaded from here. tidyr also provides the dataset. You may directly get the dataset after you load tidyr from tidyverse. The variable description can be found from tidyr documentations.\n\nlibrary(tidyverse)\nwho\n#> # A tibble: 7,240 × 60\n#>    country  iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544\n#>    <chr>    <chr> <chr> <int>       <int>        <int>        <int>        <int>\n#>  1 Afghani… AF    AFG    1980          NA           NA           NA           NA\n#>  2 Afghani… AF    AFG    1981          NA           NA           NA           NA\n#>  3 Afghani… AF    AFG    1982          NA           NA           NA           NA\n#>  4 Afghani… AF    AFG    1983          NA           NA           NA           NA\n#>  5 Afghani… AF    AFG    1984          NA           NA           NA           NA\n#>  6 Afghani… AF    AFG    1985          NA           NA           NA           NA\n#>  7 Afghani… AF    AFG    1986          NA           NA           NA           NA\n#>  8 Afghani… AF    AFG    1987          NA           NA           NA           NA\n#>  9 Afghani… AF    AFG    1988          NA           NA           NA           NA\n#> 10 Afghani… AF    AFG    1989          NA           NA           NA           NA\n#> # … with 7,230 more rows, and 52 more variables: new_sp_m4554 <int>,\n#> #   new_sp_m5564 <int>, new_sp_m65 <int>, new_sp_f014 <int>,\n#> #   new_sp_f1524 <int>, new_sp_f2534 <int>, new_sp_f3544 <int>,\n#> #   new_sp_f4554 <int>, new_sp_f5564 <int>, new_sp_f65 <int>,\n#> #   new_sn_m014 <int>, new_sn_m1524 <int>, new_sn_m2534 <int>,\n#> #   new_sn_m3544 <int>, new_sn_m4554 <int>, new_sn_m5564 <int>,\n#> #   new_sn_m65 <int>, new_sn_f014 <int>, new_sn_f1524 <int>, …\n\nBased on the description of varaibles, we understand that\n\ncountry, iso2, iso3 are all refered to country names (and thus they are redundant).\nColumns after year, like new_sp_m014 etc., are counts of new TB cases recorded by groups. The code has three parts, most of which are separated by _ (but there are some exceptions).\n\nThe first part is always new.\nThe second part is a code for method of diagnosis:\n\nrel = relapse,\nsn = negative pulmonary smear,\nsp = positive pulmonary smear,\nep = extrapulmonary.\n\nThe third part is a code for gender (f = female, m = male) and a code for age group:\n\n014 = 0-14 yrs of age,\n1524 = 15-24 years of age,\n2534 = 25 to 34 years of age,\n3544 = 35 to 44 years of age,\n4554 = 45 to 54 years of age,\n5564 = 55 to 64 years of age,\n65 = 65 years of age or older\n\n\n\nTherefore to clean the data, we need the following steps.\n\nExample 8.9 Gather together all the columns from new_sp_m014 to newrel_f65.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nwholonger <- who %>% pivot_longer(cols=5:60, names_to='group', values_to='counts')\n\n\n\n\n\nThen we use stringr::str_replace() to replace newrel by new_rel.\n\nwholonger2 <- wholonger %>% mutate(key=str_replace(group, 'newrel', 'new_rel'))\n\n\nExample 8.10 Parse the column group into columns.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nwholonger3 <- wholonger2 %>% \n        separate(key, into=c('new', 'type', 'genderage'), sep='_') %>% \n        separate(genderage, into=c('gender', 'age'), sep=1)\n\n\n\n\n\n\nExample 8.11 Pick the columns that matters.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntidywho <- wholonger3[c('country', 'year', 'type', 'gender', 'age', 'counts')]\n\n\n\n\n\nWe could use the pipe symbol to connect all the above steps.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntidywho <- who %>% \n    pivot_longer(cols=5:60, names_to='group', values_to='counts') %>% \n    mutate(key=str_replace(group, 'newrel', 'new_rel')) %>% \n    separate(key, into=c('new', 'type', 'genderage'), sep='_') %>% \n    separate(genderage, into=c('gender', 'age'), sep=1) %>% \n    select('country', 'year', 'type', 'gender', 'age', 'counts')"
  },
  {
    "objectID": "contents/4/04-.html#basic-pandas",
    "href": "contents/4/04-.html#basic-pandas",
    "title": "4  Package: pandas",
    "section": "4.1 Basic pandas",
    "text": "4.1 Basic pandas\n\n4.1.1 Series and DataFrame\nA Series is a 1-d array-like object which has index. The default index is starting from 0. You may change the index to be something assigned by you. Thus it can be treated as a generalization of a dict.\n\nobj = pd.Series([3, 1, 2, 4])\nobj\n\n0    3\n1    1\n2    2\n3    4\ndtype: int64\n\n\n\nobj2 = pd.Series([3, 1, 2, 4], index=['a', 'b', 'c', 'd'])\nobj2\n\na    3\nb    1\nc    2\nd    4\ndtype: int64\n\n\n\ndata3 = {'a': 3, 'b': 1, 'c': 2, 'd': 4}\nobj3 = pd.Series(data3)\nobj3\n\na    3\nb    1\nc    2\nd    4\ndtype: int64\n\n\nA DataFrame represents a rectangular table of data and contains an ordered collection of columns, each of which can be a different value type. The DataFrame has both a row and column index; it can be thought of as a dict of Series all sharing the same index. When displaying a DataFrame, we may use .head() to just display the first few rows for efficicy.\n\nimport pandas as pd\n\ndata = {'a': [1, 2, 3, 4, 5, 6, 7],\n        'b': [1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n        'c': ['a', 'b', 'c', 'd', 'e', 'f', 'g']}\ndf = pd.DataFrame(data)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      1.1\n      a\n    \n    \n      1\n      2\n      2.1\n      b\n    \n    \n      2\n      3\n      3.1\n      c\n    \n    \n      3\n      4\n      4.1\n      d\n    \n    \n      4\n      5\n      5.1\n      e\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe may use the setting columns= or index= as well as the methods .rename(columns=, index=) to change the column names and the index names. See the following example.\n\nimport numpy as np\nimport pandas as pd\ndata = pd.DataFrame(np.arange(16).reshape((4, 4)),\n                    index=['Ohio', 'Colorado', 'Utah', 'New York'],\n                    columns=['one', 'two', 'three', 'four'])\n\n\n\n\n\n4.1.2 Accessing data\n\nA column in a DataFrame can be retrieved as a Series either by dict-like notation or by attribute. What one gets from this is a Series object.\n\ndict-like notation: df['a']\nby attribute: df.a. Note that if the name of the column is not suitable for attribute names, this method doesn’t work.\n\nRows are retrieved by .loc if using the row index, and by .iloc if using the row number.\n\n\n\n4.1.3 Updating data\n\nAssign values to a column of a DataFrame will update that column. If the column doesn’t exist, new column will be created.\nWhen assign values with non-existent row index, that part of the data will be discarded.\nAny time if there are no values with a specific column and row, it will show as NaN.\n\n\nExample 4.1  \n\nimport pandas as pd\n\ndata = {'a': [1, 2, 3, 4],\n        'b': [1.1, 2.1, 3.1, 4.1],\n        'c': ['a', 'b', 'c', 'd']}\ndf = pd.DataFrame(data)\n\nnewcol = {1: 'good', 3: 'better', 5: 'best'}\ndf['d'] = pd.Series(newcol)\ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      d\n    \n  \n  \n    \n      0\n      1\n      1.1\n      a\n      NaN\n    \n    \n      1\n      2\n      2.1\n      b\n      good\n    \n    \n      2\n      3\n      3.1\n      c\n      NaN\n    \n    \n      3\n      4\n      4.1\n      d\n      better\n    \n  \n\n\n\n\n\n\n\n4.1.4 Indexing, Selection, and Filtering\n\nSeries indexing (obj[...]) works analogously to NumPy array indexing, except you can use the Series’s index values instead of only integers.\nWe can use logical expresssion to filter DataFrame.\n\n\nimport pandas as pd\n\ndata = pd.DataFrame(np.arange(16).reshape((4, 4)),\n                    index=['Ohio', 'Colorado', 'Utah', 'New York'],\n                    columns=['one', 'two', 'three', 'four'])\ndata[data['one']>5]\n\n\n\n\n\n  \n    \n      \n      one\n      two\n      three\n      four\n    \n  \n  \n    \n      Utah\n      8\n      9\n      10\n      11\n    \n    \n      New York\n      12\n      13\n      14\n      15\n    \n  \n\n\n\n\n\n.loc, .iloc\n\n\nimport pandas as pd\ndata = pd.DataFrame(np.arange(16).reshape((4, 4)),\n                    index=['Ohio', 'Colorado', 'Utah', 'New York'],\n                    columns=['one', 'two', 'three', 'four'])\nprint(data.loc['Colorado', ['two', 'three']])\nprint(data.iloc[2, [3, 0, 1]])\n\ntwo      5\nthree    6\nName: Colorado, dtype: int32\nfour    11\none      8\ntwo      9\nName: Utah, dtype: int32\n\n\n\nSlicing with labels behaves differently than normal Python slicing in that the endpoint is inclusive.\n\n\nimport pandas as pd\n\nobj = pd.Series(np.arange(4.), index=['a', 'b', 'c', 'd'])\nobj['b':'c']\n\nb    1.0\nc    2.0\ndtype: float64\n\n\n\nReindex .reindex():\n\n\nimport pandas as pd\ndata = pd.DataFrame(np.arange(16).reshape((4, 4)),\n                    index=['Ohio', 'Colorado', 'Utah', 'New York'],\n                    columns=['one', 'two', 'three', 'four'])\n\ndata.reindex(index = ['Colorado', 'Arkansas', 'New York'],\n             columns = ['three', 'five', 'one'])\n\n\n\n\n\n  \n    \n      \n      three\n      five\n      one\n    \n  \n  \n    \n      Colorado\n      6.0\n      NaN\n      4.0\n    \n    \n      Arkansas\n      NaN\n      NaN\n      NaN\n    \n    \n      New York\n      14.0\n      NaN\n      12.0\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n.loc and .reindex are very similar to each other. The main difference between theses two is that .loc will return a view and .reindex will return a copy in most cases.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen locate data using indexes, duplicate labels will return all results.\n\n\n\n\n4.1.5 Essential functions\n\nArithmetic and Data Alignment Elements of the same index and columns will be computed. By default, if any entry is nan, the answer will be nan. You may use fill_value argument to fill the empty slots.\n\n\nExample 4.2  \n\nimport pandas as pd\nimport numpy as np\ndf1 = pd.DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))\ndf2 = pd.DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))\ndf2.loc[1, 'b'] = np.nan\n\ndf1.add(df2, fill_value=0)\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      d\n      e\n    \n  \n  \n    \n      0\n      0.0\n      2.0\n      4.0\n      6.0\n      4.0\n    \n    \n      1\n      9.0\n      5.0\n      13.0\n      15.0\n      9.0\n    \n    \n      2\n      18.0\n      20.0\n      22.0\n      24.0\n      14.0\n    \n    \n      3\n      15.0\n      16.0\n      17.0\n      18.0\n      19.0\n    \n  \n\n\n\n\n\nRelatedly, when reindexing a Series or DataFrame, you can also specify a fill_value.\n\n\n4.1.6 Function Application and Mapping\nWe may apply functions to each row/column of a DataFrame. If the function is built-in function that is compatible with DataFrame, you can directly call the function that it will be applied automatically to each row/column. If it is not, we can call apply to get the desired result.\n\nExample 4.3  \n\nimport pandas as pd\ndata = pd.DataFrame(np.random.rand(4, 4),\n                    index=['Ohio', 'Colorado', 'Utah', 'New York'],\n                    columns=['one', 'two', 'three', 'four'])\n\nf = lambda x: x.max() - x.min()\n\nprint(data.apply(f))\nprint(data.apply(f, axis='columns'))\n\none      0.189348\ntwo      0.700152\nthree    0.330421\nfour     0.766805\ndtype: float64\nOhio        0.719334\nColorado    0.576251\nUtah        0.236155\nNew York    0.364350\ndtype: float64\n\n\n\nWe can use more complicated function to get more complicated result.\n\nExample 4.4  \n\nimport pandas as pd\ndata = pd.DataFrame(np.random.rand(4, 4),\n                    index=['Ohio', 'Colorado', 'Utah', 'New York'],\n                    columns=['one', 'two', 'three', 'four'])\n\nf = lambda x: pd.Series([x.max(), x.min()], index=['max', 'min'])\n\nprint(data.apply(f))\n\n          one       two     three      four\nmax  0.868566  0.872288  0.938442  0.697079\nmin  0.248375  0.022153  0.138255  0.134982\n\n\n\n\n\n4.1.7 Sorting and Ranking\n\n.sort_values(by=)\n.rank(ascending=, method=)\n\n\n\n4.1.8 Summarizing and Computing Descriptive Statistics\n\nsum, cumsum\nmean, median\n.describe()\n.cov, .corr\n\n\n\n4.1.9 Unique Values, Value Counts, and Membership\n\nunique\nvalue_counts\n\n\n\n4.1.10 Reading and Writing Data in Text Format\n\nread_csv\nread_excel\ndf.to_csv\n\n\n\n4.1.11 Copies and views\n\ninplace"
  },
  {
    "objectID": "contents/4/04-.html#data-cleaning",
    "href": "contents/4/04-.html#data-cleaning",
    "title": "4  Package: pandas",
    "section": "4.2 Data cleaning",
    "text": "4.2 Data cleaning\n\n4.2.1 Handling Missing Data\n\nnp.nan, pd.NA\npd.isnull(), np.isnan()\ndropna, fillna\n\n\nExample 4.5  \n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan], \n                     [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]])\ncleaned = data.dropna()\ncleanedrow = data.dropna(how='all')\ndata\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      1.0\n      6.5\n      3.0\n    \n    \n      1\n      1.0\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      6.5\n      3.0\n    \n  \n\n\n\n\n\ncleaned\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      1.0\n      6.5\n      3.0\n    \n  \n\n\n\n\n\ncleanedrow\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      1.0\n      6.5\n      3.0\n    \n    \n      1\n      1.0\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      6.5\n      3.0\n    \n  \n\n\n\n\n\ndata[4] = np.nan\ncleaned1 = data.dropna(axis=1, how='all')\ncleanedthresh = data.dropna(thresh=2)\ndata\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      4\n    \n  \n  \n    \n      0\n      1.0\n      6.5\n      3.0\n      NaN\n    \n    \n      1\n      1.0\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      6.5\n      3.0\n      NaN\n    \n  \n\n\n\n\n\ncleaned1\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      1.0\n      6.5\n      3.0\n    \n    \n      1\n      1.0\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      6.5\n      3.0\n    \n  \n\n\n\n\n\ncleanedthresh\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      4\n    \n  \n  \n    \n      0\n      1.0\n      6.5\n      3.0\n      NaN\n    \n    \n      3\n      NaN\n      6.5\n      3.0\n      NaN\n    \n  \n\n\n\n\n\nfill0 = data.fillna(0)\nfilldict = data.fillna({1: 0.5, 2: -0.1})\ndata\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      4\n    \n  \n  \n    \n      0\n      1.0\n      6.5\n      3.0\n      NaN\n    \n    \n      1\n      1.0\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      6.5\n      3.0\n      NaN\n    \n  \n\n\n\n\n\nfill0\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      4\n    \n  \n  \n    \n      0\n      1.0\n      6.5\n      3.0\n      0.0\n    \n    \n      1\n      1.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      0.0\n      6.5\n      3.0\n      0.0\n    \n  \n\n\n\n\n\nfilldict\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      4\n    \n  \n  \n    \n      0\n      1.0\n      6.5\n      3.0\n      NaN\n    \n    \n      1\n      1.0\n      0.5\n      -0.1\n      NaN\n    \n    \n      2\n      NaN\n      0.5\n      -0.1\n      NaN\n    \n    \n      3\n      NaN\n      6.5\n      3.0\n      NaN\n    \n  \n\n\n\n\n\n\n\n4.2.2 Data Transformation\n\n.duplicated(), drop_duplicates()\n\n\nExample 4.6  \n\nimport numpy as np\nimport pandas as pd\n\ndata = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'], \n                     'k2': [1, 1, 2, 3, 3, 4, 4]})\ndata.drop_duplicates(['k1'], keep='last')\n\n\n\n\n\n  \n    \n      \n      k1\n      k2\n    \n  \n  \n    \n      4\n      one\n      3\n    \n    \n      6\n      two\n      4\n    \n  \n\n\n\n\n\n\npd.Series.map(), pd.DataFrame.apply()\n\n\nExample 4.7  \n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',\n                     'Pastrami', 'corned beef', 'Bacon',\n                     'pastrami', 'honey ham', 'nova lox'],\n                     'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\n\nmeat_to_animal = {\n    'bacon': 'pig',\n    'pulled pork': 'pig',\n    'pastrami': 'cow',\n    'corned beef': 'cow',\n    'honey ham': 'pig',\n    'nova lox': 'salmon'\n    }\n\ndata['animal'] = data['food'].str.lower().map(meat_to_animal)\n\ndata['food'].map(lambda x: meat_to_animal[x.lower()])\n\n0       pig\n1       pig\n2       pig\n3       cow\n4       cow\n5       pig\n6       cow\n7       pig\n8    salmon\nName: food, dtype: object\n\n\n\n\nreplace\nrename \ndescribe\npermutation\nsample\ndummy variables\n\n\n\n4.2.3 Example: Movies\nBelow we explore the MovieLens 1M datasets. You may download it from this link.\n\nimport pandas as pd\nimport numpy as np\nmnames = ['movie_id', 'title', 'genres']\nmovies = pd.read_table('assests/datasets/movies.dat', sep='::',\n                       header=None, names=mnames, engine=\"python\",\n                       encoding='ISO-8859-1')\n\nall_genres = list()\nmovies['genres'].map(lambda x: all_genres.extend(x.split('|')))\n\ngenres = pd.unique(all_genres)\n\ndummies = pd.DataFrame(np.zeros((len(movies), len(genres))), columns=genres)\n\nfor i, gen in enumerate(movies.genres):\n    indices = dummies.columns.get_indexer(gen.split('|'))\n    dummies.iloc[i, indices] = 1\n\nmovies_windic = movies.join(dummies.add_prefix('Genre_'))\n\n\n\n4.2.4 String Manipulation\nThe key idea in this section is that, all methods in pd.Series.str will be applied to each entry of the Series.\n\nExample 4.8  \n\nimport pandas as pd\nimport numpy as np\ns = pd.Series([\"A \", \" B \", \"C\", \"Aaba\", \" Baca \", np.nan, \"CABA\", \"dog\", \"cat\"])\n\ns.str.lower()\ns.str.split('a')\ns.str.len()\ns.str.strip()\ns.str.replace(\"A\", '1')\n\n0        1 \n1        B \n2         C\n3      1aba\n4     Baca \n5       NaN\n6      C1B1\n7       dog\n8       cat\ndtype: object\n\n\n\n\nExample 4.9 We could also use .str to play with column names and row indexes.\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.randn(3, 2),\n                  columns=[\" Column A \", \" Column B \"], index=range(3))\n\ndf.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\ndf\n\n\n\n\n\n  \n    \n      \n      column_a\n      column_b\n    \n  \n  \n    \n      0\n      1.375395\n      0.738538\n    \n    \n      1\n      0.213217\n      -0.128138\n    \n    \n      2\n      -1.410801\n      -0.002241\n    \n  \n\n\n\n\n\n\n\n4.2.5 Regular expression\nRegular expressions provide a flexible way to search or match string patterns in text. A single expression, commonly called a regex, is a string formed according to the regular expression language. Python’s built-in re module is responsible for applying regular expressions to strings.\nFor details of the regular expression language in Python, please read the official documents from here. There are also many great websites for learning regex. This is one example.\nWe will briefly mentioned a few rules here.\n\n.: matches any character except a newline.\n\\d: matches any digit. It is the same as [0-9].\n\\w: matches any alphabatic or numeric character. It is the same as [a-zA-Z0-9_].\n\\s: matches any whitespaces. It is the same as [\\t\\n\\r\\f\\v].\n*: Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible.\n+: Causes the resulting RE to match 1 or more repetitions of the preceding RE, as many repetitions as are possible.\n?: Causes the resulting RE to match 0 or 1 repetitions of the preceding RE.\n*?, +?, ??: The *, +, and ? qualifiers are all greedy; they match as much text as possible. Adding ? after the qualifier makes it perform the match in non-greedy or minimal fashion; as few characters as possible will be matched.\n{m}: Specifies that exactly m copies of the previous RE should be matched.\n{m,n}: Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as many repetitions as possible.\n{m,n}?: Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as few repetitions as possible.\n[]: Used to indicate a set of characters.\n(): set groups.\n\n\nExample 4.10  \n\nimport re\ntext = \"foo bar\\t baz \\tqux\"\npattern = '\\s+'\nregex = re.compile(pattern)\nregex.split(text)\n\n['foo', 'bar', 'baz', 'qux']\n\n\n\n\n.match()\n.search()\n.findall()\n.split()\n.sub()\n\nWe can use () to specify groups, and use .groups() to get access to the results.\n\nExample 4.11  \n\nimport re\npattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})'\nregex = re.compile(pattern, flags=re.IGNORECASE)\nm = regex.match('wesm@bright.net')\nm.groups()\n\n('wesm', 'bright', 'net')\n\n\n\nTo use regex to DataFrame and Series, you may directly apply .match, .findall, .replace after .str, with the regex pattern as one of the arguments.\n.extract is a method that is not from re. It is used to extract the matched groups and make them as a DataFrame.\n\nExample 4.12  \n\nimport pandas as pd\nimport numpy as np\nmnames = ['movie_id', 'title', 'genres']\nmovies = pd.read_table('assests/datasets/movies.dat', sep='::',\n                       header=None, names=mnames, engine=\"python\",\n                       encoding='ISO-8859-1')\n\npattern = r'([a-zA-Z0-9_\\s,.?:;\\']+)\\((\\d{4})\\)'\nmovies = movies.join(movies.title.str.extract(pattern).rename(columns={0: 'movie title', 1: 'year'}))"
  },
  {
    "objectID": "contents/4/04-.html#data-wrangling",
    "href": "contents/4/04-.html#data-wrangling",
    "title": "4  Package: pandas",
    "section": "4.3 Data Wrangling",
    "text": "4.3 Data Wrangling\n\n4.3.1 Hierarchical indexing\nPandas support a more complex indexing system, that the index may have multiple levels. See the following example.\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.Series(np.random.randn(9),\n                 index = [['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'],\n                          [1, 2, 3, 1, 2, 3, 1, 2, 3]])\ndata\n\na  1   -0.802479\n   2    0.759039\n   3   -1.171718\nb  1   -0.210611\n   2    0.321742\nc  3   -1.160436\n   1    0.516246\nd  2    0.397977\n   3    0.062464\ndtype: float64\n\n\nYou may look at the Series using different levels of indexes.\n\ndata['a']\n\n1   -0.802479\n2    0.759039\n3   -1.171718\ndtype: float64\n\n\n\ndata.loc[:, 2]\n\na    0.759039\nb    0.321742\nd    0.397977\ndtype: float64\n\n\nYou may use groupby to group by levels and do calculations related to levels. More .groupby() will be discussed in the next section.\n\ndata.groupby(level=1).sum()\n\n1   -0.496844\n2    1.478758\n3   -2.269690\ndtype: float64\n\n\n\nFrom the example above, you may notice that the 2-level hierarchical indexing for a Series works very similar to a DataFrame. In fact, you may translate it back and forth between a 2-level indexing Series and a DataFrame.\n\ndf = data.unstack()\ndf\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n    \n  \n  \n    \n      a\n      -0.802479\n      0.759039\n      -1.171718\n    \n    \n      b\n      -0.210611\n      0.321742\n      NaN\n    \n    \n      c\n      0.516246\n      NaN\n      -1.160436\n    \n    \n      d\n      NaN\n      0.397977\n      0.062464\n    \n  \n\n\n\n\n\ndf.stack()\n\na  1   -0.802479\n   2    0.759039\n   3   -1.171718\nb  1   -0.210611\n   2    0.321742\nc  1    0.516246\n   3   -1.160436\nd  2    0.397977\n   3    0.062464\ndtype: float64\n\n\nFor DataFrame the index for both axes can be multiindex. The usual indexing way can be used if you want to start from the first level of the index. The more specific method to extract data is .xs.\n\nExample 4.13  \n\nimport pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n    },\n    index=[0, 1, 2, 3],\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"A\": [\"A4\", \"A5\", \"A6\", \"A7\"],\n        \"B\": [\"B4\", \"B5\", \"B6\", \"B7\"],\n        \"C\": [\"C4\", \"C5\", \"C6\", \"C7\"],\n        \"D\": [\"D4\", \"D5\", \"D6\", \"D7\"],\n    },\n    index=[4, 5, 6, 7],\n)\n\ndf = pd.concat([df1, df2], keys=['x', 'y'])\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      x\n      0\n      A0\n      B0\n      C0\n      D0\n    \n    \n      1\n      A1\n      B1\n      C1\n      D1\n    \n    \n      2\n      A2\n      B2\n      C2\n      D2\n    \n    \n      3\n      A3\n      B3\n      C3\n      D3\n    \n    \n      y\n      4\n      A4\n      B4\n      C4\n      D4\n    \n    \n      5\n      A5\n      B5\n      C5\n      D5\n    \n    \n      6\n      A6\n      B6\n      C6\n      D6\n    \n    \n      7\n      A7\n      B7\n      C7\n      D7\n    \n  \n\n\n\n\n\ndf['A']\n\nx  0    A0\n   1    A1\n   2    A2\n   3    A3\ny  4    A4\n   5    A5\n   6    A6\n   7    A7\nName: A, dtype: object\n\n\n\ndf.loc['x']\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      A0\n      B0\n      C0\n      D0\n    \n    \n      1\n      A1\n      B1\n      C1\n      D1\n    \n    \n      2\n      A2\n      B2\n      C2\n      D2\n    \n    \n      3\n      A3\n      B3\n      C3\n      D3\n    \n  \n\n\n\n\n\ndf.loc['x',3]\n\nA    A3\nB    B3\nC    C3\nD    D3\nName: (x, 3), dtype: object\n\n\n\ndf.xs(3, level=1, drop_level=False)\n\n\n\n\n\n  \n    \n      \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      x\n      3\n      A3\n      B3\n      C3\n      D3\n    \n  \n\n\n\n\n\n\n\n4.3.2 Combining and Merging Datasets\n\n4.3.2.1 merge()\nMerge combines datasets by linking rows using one or more keys. This is from relational databases (e.g., SQL-based).\nHere are some examples.\n\nExample 4.14  \n\nimport pandas as pd\ndf1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n                    'data1': range(7)})\ndf2 = pd.DataFrame({'key': ['a', 'b', 'd'], 'data2': range(3)})\n\nThe two DataFrames are displayed as follows.\n\ndf1\n\n\n\n\n\n  \n    \n      \n      key\n      data1\n    \n  \n  \n    \n      0\n      b\n      0\n    \n    \n      1\n      b\n      1\n    \n    \n      2\n      a\n      2\n    \n    \n      3\n      c\n      3\n    \n    \n      4\n      a\n      4\n    \n    \n      5\n      a\n      5\n    \n    \n      6\n      b\n      6\n    \n  \n\n\n\n\n\ndf2\n\n\n\n\n\n  \n    \n      \n      key\n      data2\n    \n  \n  \n    \n      0\n      a\n      0\n    \n    \n      1\n      b\n      1\n    \n    \n      2\n      d\n      2\n    \n  \n\n\n\n\n\npd.merge(df1, df2, on='key')\n\n\n\n\n\n  \n    \n      \n      key\n      data1\n      data2\n    \n  \n  \n    \n      0\n      b\n      0\n      1\n    \n    \n      1\n      b\n      1\n      1\n    \n    \n      2\n      b\n      6\n      1\n    \n    \n      3\n      a\n      2\n      0\n    \n    \n      4\n      a\n      4\n      0\n    \n    \n      5\n      a\n      5\n      0\n    \n  \n\n\n\n\nIf the column names are different in each object, you can specify them separately.\n\ndf3 = pd.DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n                    'data1': range(7)})\ndf4 = pd.DataFrame({'rkey': ['a', 'b', 'd'],\n                    'data2': range(3)})\npd.merge(df3, df4, left_on='lkey', right_on='rkey')\n\n\n\n\n\n  \n    \n      \n      lkey\n      data1\n      rkey\n      data2\n    \n  \n  \n    \n      0\n      b\n      0\n      b\n      1\n    \n    \n      1\n      b\n      1\n      b\n      1\n    \n    \n      2\n      b\n      6\n      b\n      1\n    \n    \n      3\n      a\n      2\n      a\n      0\n    \n    \n      4\n      a\n      4\n      a\n      0\n    \n    \n      5\n      a\n      5\n      a\n      0\n    \n  \n\n\n\n\n\nBy default merge does an inner join, that the keys in the result are the interesection found in both tables. Below are different types of merge. To specify the method for merge, the option is how.\n\ninner\nleft\nright\nouter\n\nLet’s see the following examples.\n\n\n\ndf1 = pd.DataFrame({'Key': [1, 2], 'A': [0, 2], 'B': [1, 3]})\ndf1\n\n\n\n\n\n  \n    \n      \n      Key\n      A\n      B\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n    \n    \n      1\n      2\n      2\n      3\n    \n  \n\n\n\n\n\n\n\ndf2 = pd.DataFrame({'Key': [1, 3], 'C': [0, 2], 'D': [1, 3]})\ndf2\n\n\n\n\n\n  \n    \n      \n      Key\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n    \n    \n      1\n      3\n      2\n      3\n    \n  \n\n\n\n\n\n\n\n\n\npd.merge(df1, df2, on='Key', how='inner')\n\n\n\n\n\n  \n    \n      \n      Key\n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      0\n      1\n    \n  \n\n\n\n\n\n\n\npd.merge(df1, df2, on='Key', how='outer')\n\n\n\n\n\n  \n    \n      \n      Key\n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      0.0\n      1.0\n      0.0\n      1.0\n    \n    \n      1\n      2\n      2.0\n      3.0\n      NaN\n      NaN\n    \n    \n      2\n      3\n      NaN\n      NaN\n      2.0\n      3.0\n    \n  \n\n\n\n\n\n\n\n\n\npd.merge(df1, df2, on='Key', how='left')\n\n\n\n\n\n  \n    \n      \n      Key\n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      0.0\n      1.0\n    \n    \n      1\n      2\n      2\n      3\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\n\npd.merge(df1, df2, on='Key', how='right')\n\n\n\n\n\n  \n    \n      \n      Key\n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1\n      0.0\n      1.0\n      0\n      1\n    \n    \n      1\n      3\n      NaN\n      NaN\n      2\n      3\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf a key combination appears more than once in both tables, the resulting table will have the Cartesian product of the associated data. Here is a very basic example with one unique key combination.\n\ndf1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\n                    'data1': range(6)})\ndf2 = pd.DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],\n                    'data2': range(5)})\npd.merge(df1, df2, on='key', how='left')\n\n\n\n\n\n  \n    \n      \n      key\n      data1\n      data2\n    \n  \n  \n    \n      0\n      b\n      0\n      1.0\n    \n    \n      1\n      b\n      0\n      3.0\n    \n    \n      2\n      b\n      1\n      1.0\n    \n    \n      3\n      b\n      1\n      3.0\n    \n    \n      4\n      a\n      2\n      0.0\n    \n    \n      5\n      a\n      2\n      2.0\n    \n    \n      6\n      c\n      3\n      NaN\n    \n    \n      7\n      a\n      4\n      0.0\n    \n    \n      8\n      a\n      4\n      2.0\n    \n    \n      9\n      b\n      5\n      1.0\n    \n    \n      10\n      b\n      5\n      3.0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the merge keys in a DataFrame is in its index instead of column(s), we could pass left_index=True or right_index=True or both instead of setting left_on/right_on/on.\n\n\n\nExample 4.15 If we want to really create a Cartesian product, we may use the option how='cross'. For example, we would like to generate a deck of cards, we may use the following codes.\n\nsuit = pd.DataFrame({'suit': ['spades', 'hearts', 'clubs', 'diamonds']})\nface = pd.DataFrame({'face': list(range(1, 14))})\ndeck = pd.merge(suit, face, how='cross')\n\n\n\n\n4.3.2.2 concat()\nThe concat() function (in the main pandas namespace) performs concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.\n\nimport pandas as pd\n\ndf1 = pd.DataFrame(\n    {\n        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n    },\n    index=[0, 1, 2, 3],\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"A\": [\"A4\", \"A5\", \"A6\", \"A7\"],\n        \"B\": [\"B4\", \"B5\", \"B6\", \"B7\"],\n        \"C\": [\"C4\", \"C5\", \"C6\", \"C7\"],\n        \"D\": [\"D4\", \"D5\", \"D6\", \"D7\"],\n    },\n    index=[4, 5, 6, 7],\n)\n\ndf3 = pd.DataFrame(\n    {\n        \"A\": [\"A8\", \"A9\", \"A10\", \"A11\"],\n        \"B\": [\"B8\", \"B9\", \"B10\", \"B11\"],\n        \"C\": [\"C8\", \"C9\", \"C10\", \"C11\"],\n        \"D\": [\"D8\", \"D9\", \"D10\", \"D11\"],\n    },\n    index=[8, 9, 10, 11],\n)\n\npd.concat([df1, df2, df3], keys=['x', 'y', 'z'])\n\n\n\n\n\n  \n    \n      \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      x\n      0\n      A0\n      B0\n      C0\n      D0\n    \n    \n      1\n      A1\n      B1\n      C1\n      D1\n    \n    \n      2\n      A2\n      B2\n      C2\n      D2\n    \n    \n      3\n      A3\n      B3\n      C3\n      D3\n    \n    \n      y\n      4\n      A4\n      B4\n      C4\n      D4\n    \n    \n      5\n      A5\n      B5\n      C5\n      D5\n    \n    \n      6\n      A6\n      B6\n      C6\n      D6\n    \n    \n      7\n      A7\n      B7\n      C7\n      D7\n    \n    \n      z\n      8\n      A8\n      B8\n      C8\n      D8\n    \n    \n      9\n      A9\n      B9\n      C9\n      D9\n    \n    \n      10\n      A10\n      B10\n      C10\n      D10\n    \n    \n      11\n      A11\n      B11\n      C11\n      D11\n    \n  \n\n\n\n\nThe default way of pd.concat() is vertically. Note that it will check the column names. If the column names don’t match, new columns will be created and nan values will be assigned.\nIf you want to concatenate the DataFrame horizontally you need to add axis=1 option. Similarly, row index will be checked before concatenating. See the following example.\n\nExample 4.16  \n\npd.concat([df1, df2, df3], axis=1)\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      A\n      B\n      C\n      D\n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      A0\n      B0\n      C0\n      D0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      A1\n      B1\n      C1\n      D1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      A2\n      B2\n      C2\n      D2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      A3\n      B3\n      C3\n      D3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      A4\n      B4\n      C4\n      D4\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      NaN\n      NaN\n      NaN\n      NaN\n      A5\n      B5\n      C5\n      D5\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6\n      NaN\n      NaN\n      NaN\n      NaN\n      A6\n      B6\n      C6\n      D6\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7\n      NaN\n      NaN\n      NaN\n      NaN\n      A7\n      B7\n      C7\n      D7\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      8\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      A8\n      B8\n      C8\n      D8\n    \n    \n      9\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      A9\n      B9\n      C9\n      D9\n    \n    \n      10\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      A10\n      B10\n      C10\n      D10\n    \n    \n      11\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      A11\n      B11\n      C11\n      D11\n    \n  \n\n\n\n\n\n\nExample 4.17 Consider the deck example from Example 4.15. This time we would like to use pd.concat() to get the result.\n\nsuitlist = ['spades', 'hearts', 'clubs', 'diamonds']\nfacelist = list(range(1, 14))\ndecklist = [pd.DataFrame({'suit': suit, 'face': facelist}) for suit in suitlist]\ndeck = pd.concat(decklist, ignore_index=True)"
  },
  {
    "objectID": "contents/4/04-.html#data-aggregation-and-group-operations",
    "href": "contents/4/04-.html#data-aggregation-and-group-operations",
    "title": "4  Package: pandas",
    "section": "4.4 Data Aggregation and Group Operations",
    "text": "4.4 Data Aggregation and Group Operations\n\n4.4.1 split-apply-combine model\nWe would like to apply group operations based on the split-apply-combine model.\n\nIn the first stage of the process, data contained in a pandas object is split into groups based on one or more keys that you provide. We then use .groupby(keys) to perform the split step. The result is a grouped groupby object.\nOnce this is done, a function is applied to each group, producing a new value.\nFinally the results of all those function applications are combined into a result object. We may apply groupby functions directly as methods to groupby objects.The result is the combined result object.\n\n\nExample 4.18  \n\nimport pandas as pd\ndf = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n                   'key2' : ['one', 'two', 'one', 'two', 'one'],\n                   'data1' : np.random.randn(5),\n                   'data2' : np.random.randn(5)})\ndf\n\n\n\n\n\n  \n    \n      \n      key1\n      key2\n      data1\n      data2\n    \n  \n  \n    \n      0\n      a\n      one\n      0.873122\n      0.561476\n    \n    \n      1\n      a\n      two\n      -0.426869\n      -1.433012\n    \n    \n      2\n      b\n      one\n      1.756470\n      0.352796\n    \n    \n      3\n      b\n      two\n      0.754264\n      -2.418179\n    \n    \n      4\n      a\n      one\n      -0.452852\n      -0.950064\n    \n  \n\n\n\n\nNow we want to group data1 in df by key1.\n\ngrouped = df['data1'].groupby(df['key1'])\ngrouped\n\n<pandas.core.groupby.generic.SeriesGroupBy object at 0x000002020D272F40>\n\n\nWhat we get is a groupby object and we could apply group functions to it.\nThe method to look at each group is .get_group.\n\ngrouped.get_group('a')\n\n0    0.873122\n1   -0.426869\n4   -0.452852\nName: data1, dtype: float64\n\n\nWe may directly apply some group functions to the groupby object.\n\ngrouped.mean()\n\nkey1\na   -0.002200\nb    1.255367\nName: data1, dtype: float64\n\n\n\ngrouped.size()\n\nkey1\na    3\nb    2\nName: data1, dtype: int64\n\n\nWe could iterate over groups.\n\nfor name, group in grouped:\n    print('name', name)\n    print('group', group)\n\nname a\ngroup 0    0.873122\n1   -0.426869\n4   -0.452852\nName: data1, dtype: float64\nname b\ngroup 2    1.756470\n3    0.754264\nName: data1, dtype: float64\n\n\nWe could convert the group object into list and dictionary.\n\nlist(grouped)\n\n[('a',\n  0    0.873122\n  1   -0.426869\n  4   -0.452852\n  Name: data1, dtype: float64),\n ('b',\n  2    1.756470\n  3    0.754264\n  Name: data1, dtype: float64)]\n\n\n\ndict(list(grouped))\n\n{'a': 0    0.873122\n 1   -0.426869\n 4   -0.452852\n Name: data1, dtype: float64,\n 'b': 2    1.756470\n 3    0.754264\n Name: data1, dtype: float64}\n\n\n\n\n\n4.4.2 More aggregation functions\n\n.describe()\n.count()\n.sum()\n.mean()\n.median\n.std(), .var()\n.min(), .max()\n.prod()\nfirst(), .last()\n.agg()\n\n\n\n4.4.3 Some examples\n\nExample 4.19 Consider the following DataFrame.\n\nimport pandas as pd\ndf = pd.DataFrame({'location': ['East', 'East', 'East', 'East',\n                                'West', 'West', 'West', 'West'],\n                   'data': np.random.randn(8)},\n                   index=['Ohio', 'New York', 'Vermont', 'Florida',\n                          'Oregon', 'Nevada', 'California', 'Idaho'])\ndf.loc[['Vermont', 'Nevada', 'Idaho'], 'data'] = np.nan\n\nWe would like to fill in NA values with the mean from each group.\n\ndf.groupby('location').apply(lambda x: x.fillna(x.mean()))\n\nC:\\Users\\Xinli\\AppData\\Local\\Temp\\ipykernel_29412\\2040193686.py:1: FutureWarning:\n\nDropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n\n\n\n\n\n\n\n  \n    \n      \n      location\n      data\n    \n  \n  \n    \n      Ohio\n      East\n      -1.063793\n    \n    \n      New York\n      East\n      0.419128\n    \n    \n      Vermont\n      East\n      0.123220\n    \n    \n      Florida\n      East\n      1.014324\n    \n    \n      Oregon\n      West\n      -0.821446\n    \n    \n      Nevada\n      West\n      -0.185755\n    \n    \n      California\n      West\n      0.449936\n    \n    \n      Idaho\n      West\n      -0.185755\n    \n  \n\n\n\n\nWe could also fill in NA values with predefined values, similar to the non-groupby case.\n\ndf.groupby('location').apply(lambda x: x.fillna({'East': 0.1,\n                                                 'West': -0.5}[x.name]))\n\n\n\n\n\n  \n    \n      \n      location\n      data\n    \n  \n  \n    \n      Ohio\n      East\n      -1.063793\n    \n    \n      New York\n      East\n      0.419128\n    \n    \n      Vermont\n      East\n      0.100000\n    \n    \n      Florida\n      East\n      1.014324\n    \n    \n      Oregon\n      West\n      -0.821446\n    \n    \n      Nevada\n      West\n      -0.500000\n    \n    \n      California\n      West\n      0.449936\n    \n    \n      Idaho\n      West\n      -0.500000"
  },
  {
    "objectID": "contents/4/04-.html#exercises",
    "href": "contents/4/04-.html#exercises",
    "title": "4  Package: pandas",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\nMost problems are based on [1].\n\nExercise 4.1 Please use the following code to generate a series ser, and then finish the following tasks.\n\nimport pandas as pd\nimport numpy as np\n\n\nmylist = list('abcedfghijklmnopqrstuvwxyz')\nmyarr = np.arange(26)\nmydict = dict(zip(mylist, myarr))\nser = pd.Series(mydict)\n\n\nConvert the series ser into a dataframe df with its index as another column on the dataframe.\nPick the two columns of df and set them into two serieses ser1 and ser2.\nCombine two series ser1 and ser2 to form a new dataframe newdf, and name their columns ser1 and ser2.\n\n\n\nExercise 4.2 Consider two serieses ser1 and ser2. You may use the following ser1 and ser2 as an example. The output of each questions below should be a series. You may want to learn the following commands:\n\nnp.union1d()\nnp.intersect1d()\nnp.isin()\n\n\nimport pandas as pd\n\nser1 = pd.Series([1, 2, 3, 4, 5])\nser2 = pd.Series([4, 5, 6, 7, 8])\n\n\nFind all the elements from ser1 that are also in ser2.\nFind all the elements from ser2 that are also in ser1.\nFrom ser1 remove items present in ser2.\nFind the union of ser1 and ser2.\nFind the intersection of ser1 and ser2.\nFind all the elemetns that are in either ser1 or ser2, but not both.\n\n\n\nExercise 4.3 (Some statistics) Please check the following commands and answer the following questions.\n\nnp.percentile()\n\nHow to get the minimum, 25th percentile, median, 75th, and max of a numeric series? You may use the following Series as an example.\n\nimport pandas as pd\nser = pd.Series(np.random.normal(10, 5, 25))\n\n\n\nExercise 4.4 Please use pd.Series.value_counts() to calculte the frequency counts of each unique value of the following Series.\n\nimport pandas as pd\nimport numpy as np\nser = pd.Series(np.take(list('abcdefgh'), np.random.randint(8, size=30)))\n\n\n\nExercise 4.5 Please keep the top 2 most frequent items of ser as it is and replace everything else as Other.\n\nimport pandas as pd\nimport numpy as np\nser = pd.Series(np.take(list('abcdefgh'), np.random.randint(8, size=30)))\n\n\n\nExercise 4.6 Please use pd.cut or pd.qcut to bin the Series ser into 10 equal deciles. You may use the following ser as an example.\n\nimport pandas as pd\nser = pd.Series(np.random.random(20))\n\n\n\nExercise 4.7 Consider the Series ser:\n\nimport pandas as pd\nimport numpy as np\nser = pd.Series(np.random.randint(1, 10, 7))\n\nFind the positions of numbers that are multiples of 3 from ser.\n\n\nExercise 4.8 Compute the mean of weights of each fruit.\n\nimport pandas as pd\nfruit = pd.Series(np.random.choice(['apple', 'banana', 'carrot'], 10))\nweights = pd.Series(np.linspace(1, 10, 10))\ndf = pd.DataFrame({'fruit': fruit, 'weights': weights})\n\n\n\nExercise 4.9 Consider the following DataFrame.\n\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n\n\nCheck if df has any missing values.\nPlease count the number of missing values in each column.\nPlease replace all missing values in Min.Price and Max.Price with their mean respectively.\n\n\n\n\nExercise 4.10 Replace the spaces in my_str = 'dbc deb abed gade' with the least frequent character.\n\n\nExercise 4.11 Suppress scientific notations like e-03 in df and print up to 4 numbers after decimal.\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.random(4)**10, columns=['random'])\ndf\n\n\n\n\n\n  \n    \n      \n      random\n    \n  \n  \n    \n      0\n      4.622963e-07\n    \n    \n      1\n      2.912607e-02\n    \n    \n      2\n      6.323939e-02\n    \n    \n      3\n      1.510522e-01\n    \n  \n\n\n\n\n\n\nExercise 4.12 Format the values in column random of df as percentages.\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.random(4), columns=['random'])\ndf\n\n\n\n\n\n  \n    \n      \n      random\n    \n  \n  \n    \n      0\n      0.191921\n    \n    \n      1\n      0.468266\n    \n    \n      2\n      0.048119\n    \n    \n      3\n      0.263165\n    \n  \n\n\n\n\n\n\nExercise 4.13 (Regular expressions) Please use regular expressions to finish the following tasks.\n\nMatch a string that has an a followed by zero or more b’s.\nMatch a string that has an a followed by one or more b’s.\nMatch a string that has an a followed by zero or one b.\nMatch a string that has an a followed by three b’s.\n\n\n\nExercise 4.14 (More regex) Find all words starting with a or e in a given string:\n\ntext = \"The following example creates an ArrayList with a capacity of 50 elements. Four elements are then added to the ArrayList and the ArrayList is trimmed accordingly.\"\n\n\n\nExercise 4.15 (More regex) Write a Python code to extract year, month and date from a url1:\n\nurl1= \"https://www.washingtonpost.com/news/football-insider/wp/2016/09/02/odell-beckhams-fame-rests-on-one-stupid-little-ball-josh-norman-tells-author/\"\n\n\n\nExercise 4.16 (More regex) Please use regex to parse the following str to create a dictionary.\n\ntext = r'''\n{\n    name: Firstname Lastname;\n    age: 100;\n    salary: 10000 \n}\n'''\n\n\n\nExercise 4.17 Consider the following DataFrame.\n\ndata = [['Evert van Dijk', 'Carmine-pink, salmon-pink streaks, stripes, flecks.  Warm pink, clear carmine pink, rose pink shaded salmon.  Mild fragrance.  Large, very double, in small clusters, high-centered bloom form.  Blooms in flushes throughout the season.'],\n        ['Every Good Gift', 'Red.  Flowers velvety red.  Moderate fragrance.  Average diameter 4\".  Medium-large, full (26-40 petals), borne mostly solitary bloom form.  Blooms in flushes throughout the season.'], \n        ['Evghenya', 'Orange-pink.  75 petals.  Large, very double bloom form.  Blooms in flushes throughout the season.'], \n        ['Evita', 'White or white blend.  None to mild fragrance.  35 petals.  Large, full (26-40 petals), high-centered bloom form.  Blooms in flushes throughout the season.'],\n        ['Evrathin', 'Light pink. [Deep pink.]  Outer petals white. Expand rarely.  Mild fragrance.  35 to 40 petals.  Average diameter 2.5\".  Medium, double (17-25 petals), full (26-40 petals), cluster-flowered, in small clusters bloom form.  Prolific, once-blooming spring or summer.  Glandular sepals, leafy sepals, long sepals buds.'],\n        ['Evita 2', 'White, blush shading.  Mild, wild rose fragrance.  20 to 25 petals.  Average diameter 1.25\".  Small, very double, cluster-flowered bloom form.  Blooms in flushes throughout the season.']]\n  \ndf = pd.DataFrame(data, columns = ['NAME', 'BLOOM']) \ndf \n\n\n\n\n\n  \n    \n      \n      NAME\n      BLOOM\n    \n  \n  \n    \n      0\n      Evert van Dijk\n      Carmine-pink, salmon-pink streaks, stripes, fl...\n    \n    \n      1\n      Every Good Gift\n      Red.  Flowers velvety red.  Moderate fragrance...\n    \n    \n      2\n      Evghenya\n      Orange-pink.  75 petals.  Large, very double b...\n    \n    \n      3\n      Evita\n      White or white blend.  None to mild fragrance....\n    \n    \n      4\n      Evrathin\n      Light pink. [Deep pink.]  Outer petals white. ...\n    \n    \n      5\n      Evita 2\n      White, blush shading.  Mild, wild rose fragran...\n    \n  \n\n\n\n\nPlease use regex methods to find all the () in each columns.\n\n\nExercise 4.18 Get the last two rows of df whose row sum is greater than 100.\n\nimport pandas as pd\ndf = pd.DataFrame(np.random.randint(10, 40, 60).reshape(-1, 4))\n\n\n\nExercise 4.19 The groupby object df_grouped is given below.\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n                   'price': np.random.rand(9),\n                   'taste': np.random.randint(0, 11, 9)})\n\ndf_grouped = df.groupby(['fruit'])\n\n\nGet the group belonging to apple as a DataFrame.\nFind the second largest value of taste for banana.\nCompute the mean price for every fruit.\n\n\n\nExercise 4.20 Join df1 and df2 by fruit/pazham and weight/kilo.\n\ndf1 = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n                    'weight': ['high', 'medium', 'low'] * 3,\n                    'price': np.random.randint(0, 15, 9)})\n\ndf2 = pd.DataFrame({'pazham': ['apple', 'orange', 'pine'] * 2,\n                    'kilo': ['high', 'low'] * 3,\n                    'price': np.random.randint(0, 15, 6)})"
  },
  {
    "objectID": "contents/4/04-.html#projects",
    "href": "contents/4/04-.html#projects",
    "title": "4  Package: pandas",
    "section": "4.6 Projects",
    "text": "4.6 Projects\n\nExercise 4.21 Extract the valid emails from the series emails. The regex pattern for valid emails is provided as reference.\n\nimport pandas as pd\nemails = pd.Series(['buying books at amazom.com',\n                    'rameses@egypt.com',\n                    'matt@t.co',\n                    'narendra@modi.com'])\npattern = '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,4}'\n\n\n\nExercise 4.22 Consider the following DataFrame.\n\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv', usecols=[0,1,2,3,5])\n\n\nReplace NaN with string missing in columns Manufacturer, Model and Type.\nCreate an index as a combination of these three columns.\n\n\n\nExercise 4.23 Given the following DataFrame.\n\nimport pandas as pd\ndf = pd.DataFrame({\n    'name': ['James', 'Jane', 'Melissa', 'Ed', 'Neil'],\n    'age': [30, 40, 32, 67, 43],\n    'score': ['90%', '95%', '100%', '82%', '87%'],\n    'age_missing_data': [30, 40, 32, 67, None],\n    'income':[100000, 80000, 55000, 62000, 120000]\n})\n\n\nPlease use .map to create a new column numeric_score whose value is the number version of score.\nPlease use .apply to create a new column numeric_score whose value is the number version of score.\n\n\n\nExercise 4.24 From ser = pd.Series(['Apple', 'Orange', 'Plan', 'Python', 'Money']), find the words that contain at least 2 vowels.\n\n\nExercise 4.25 Please download the given file with sample emails, and use the following code to load the file and save it to a string content.\n\nwith open('assests/datasets/test_emails.txt', 'r') as f:\n    content = f.read()\n\nPlease use regex to play with content.\n\nGet all valid email address in content, from both the header part or the body part.\nThere are two emails in content. Please get the sender’s email and the receiver’s email from content.\nPlease get the sender’s name.\nPlease get the subject of each email.\n\n\n\nExercise 4.26 The following DataFrame is given.\n\nimport pandas as pd\ndf = pd.DataFrame([\"STD, City    State\",\n                   \"33, Kolkata    West Bengal\",\n                   \"44, Chennai    Tamil Nadu\",\n                   \"40, Hyderabad    Telengana\",\n                   \"80, Bangalore    Karnataka\"],\n                   columns=['row'])\n\n\nSplit the columns into a list with 3 entries.\nMake the first row (row 0) into a header.\nCreate a new DataFrame out of the data.\n\n\n\n\n\n\n[1] Prabhakaran, S. (2018). 101 pandas exercises for data analysis."
  },
  {
    "objectID": "contents/5/05-.html#matplotlib.pyplot",
    "href": "contents/5/05-.html#matplotlib.pyplot",
    "title": "5  Visualization",
    "section": "5.1 matplotlib.pyplot",
    "text": "5.1 matplotlib.pyplot\nmatplotlib is a modern and classic plot library. Its main features are inspired by MATLAB. In this book we mostly use pyplot package from matplotlib. We use the following import convention:\n\nimport matplotlib.pyplot as plt\n\n\n5.1.1 matplotlib interface\nmatplotlib has two major application interfaces, or styles of using the library:\n\nAn explicit Axes interface that uses methods on a Figure or Axes object to create other Artists, and build a visualization step by step. You may treat this Figure object as a canvas, and Axes as plots on a canvas. There might be one or more plots on one canvas. This has also been called an object-oriented interface.\nAn implicit pyplot interface that keeps track of the last Figure and Axes created, and adds Artists to the object it thinks the user wants.\n\nHere is an example of an explicit interface.\n\nfig = plt.figure()\nax = fig.subplots()\nax.plot([1, 2, 3, 4], [0, 0.5, 1, 0.2])\n\n\n\n\nHere is an example of an implicit interface.\n\nplt.plot([1, 2, 3, 4], [0, 0.5, 1, 0.2])\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the plot is not shown, you may want to type plt.show() to force the plot being rendered. However, to make plt.show() work is related to switching matplotlib backends, and is sometimes very complicated.\n\n\nThe purpose to explicitly use fig and ax is to have more control over the configurations. The first important configuration is subplots.\n\n.subplot()\n.subplots()\n.add_subplot()\n\nPlease see the following examples.\n\nExample 5.1  \n\nplt.subplot(1, 2, 1)\nplt.plot([1, 2, 3], [0, 0.5, 0.2])\n\n\n\n\n\n\nExample 5.2  \n\nplt.subplot(1, 2, 1)\nplt.plot([1, 2, 3], [0, 0.5, 0.2])\nplt.subplot(1, 2, 2)\nplt.plot([3, 2, 1], [0, 0.5, 0.2])\n\n\n\n\n\n\nExample 5.3  \n\nfig, axs = plt.subplots(1, 2)\naxs[0].plot([1, 2, 3], [0, 0.5, 0.2])\naxs[1].plot([3, 2, 1], [0, 0.5, 0.2])\n\n\n\n\n\n\nExample 5.4  \n\nimport numpy as np\nfig = plt.figure()\nax1 = fig.add_subplot(2, 2, 1)\nax2 = fig.add_subplot(2, 2, 3)\nax3 = fig.add_subplot(1, 2, 2)\n\nax3.plot([1, 2, 3], [0, 0.5, 0.2])\n\n\n\n\nThe auguments 2, 2, 1 means that we split the figure into a 2x2 grid and the axis ax1 is in the 1st position. The rest is understood in the same way.\n\n\nExample 5.5 If you don’t explicitly initialize fig and ax, you may use plt.gcf() and plt.gca() to get the handles for further operations.\n\nplt.subplot(1, 2, 1)\nax = plt.gca()\nax.plot([1, 2, 3], [0, 0.5, 0.2])\n\nplt.subplot(1, 2, 2)\nax = plt.gca()\nax.plot([3, 2, 1], [0, 0.5, 0.2])\n\n\n\n\n\nThe purpose to explicitly use fig and ax is to have more control over the configurations. For example, when generate a figure object, we may use figsize=(3, 3) as an option to set the figure size to be 3x3. dpi is another commonly modified option.\n\nfig = plt.figure(figsize=(2, 2), dpi=50)\nplt.plot([1, 2, 3], [0, 0.5, 0.2])\n\n\n\n\nIf you would like to change this setting later, you may use the following command before plotting.\n\nfig.set_size_inches(10, 10)\nfig.set_dpi(300)\nplt.plot([1, 2, 3], [0, 0.5, 0.2])\n\n\n\n\nYou may use fig.savefig('filename.png') to save the image into a file.\n\n\n5.1.2 Downstream packages\nThere are multiple packages depending on matplotlib to provide plotting. For example, you may directly plot from a Pandas DataFrame or a Pandas Series.\n\nExample 5.6  \n\nimport pandas as pd\nimport numpy as np\ns = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))\ns.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ndf = pd.DataFrame(np.random.randn(10, 4).cumsum(0),\n                  columns=['A', 'B', 'C', 'D'],\n                  index=np.arange(0, 100, 10))\ndf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\n5.1.3 plotting\n\n5.1.3.1 plt.plot()\nThis is the command for line plotting. You may use linestyle='--' and color='g' to control the line style and color. The style can be shortened as g--.\nHere is a list of commonly used linestyles and colors.\n\nline styles\n\nsolid or -\ndashed or --\ndashdot or -.\ndotted or :\n\nmarker styles\n\no as circle markers\n+ as plusses\n^ as triangles\ns as squares\n\ncolors\n\nb as blue\ng as green\nr as red\nk as black\nw as white\n\n\nThe input of plt.plot() is two lists x and y. If there is only one list inputed, that one will be recognized as y and the index of elements of y will be used as the dafault x.\n\nExample 5.7  \n\nplt.plot(np.random.randn(30).cumsum(), color='r', linestyle='--', marker='o')\n\n\n\n\nYou may compare it with this Example for the purpose of seaborn from next Section.\n\n\n\n5.1.3.2 plt.bar() and plt.barh()\nThe two commands make vertical and horizontal bar plots, respectively. ::: {#exm-}\n\nimport pandas as pd\ndata = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))\n\nfig, axes = plt.subplots(2, 1)\naxes[0].bar(x=data.index, height=data, color='k', alpha=0.7)\naxes[1].barh(y=data.index, width=data, color='b', alpha=0.7)\n\n<BarContainer object of 16 artists>\n\n\n\n\n\nWe may also directly plot the bar plot from the Series.\n\nfig, axes = plt.subplots(2, 1)\ndata.plot.bar(ax=axes[0], color='k', alpha=0.7)\ndata.plot.barh(ax=axes[1], color='b', alpha=0.7)\n\n<AxesSubplot:>\n\n\n\n\n\n:::\nWith a DataFrame, bar plots group the values in each row together in a group in bars. This is easier if we directly plot from the DataFrame.\n\nExample 5.8  \n\ndf = pd.DataFrame(np.random.rand(6, 4),\n                  index=['one', 'two', 'three', 'four', 'five', 'six'],\n                  columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))\ndf\n\n\n\n\n\n  \n    \n      Genus\n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      one\n      0.164489\n      0.152957\n      0.204993\n      0.484466\n    \n    \n      two\n      0.208928\n      0.763101\n      0.706756\n      0.454433\n    \n    \n      three\n      0.823657\n      0.639983\n      0.765202\n      0.863337\n    \n    \n      four\n      0.692975\n      0.743479\n      0.884414\n      0.730369\n    \n    \n      five\n      0.358668\n      0.678043\n      0.032361\n      0.184725\n    \n    \n      six\n      0.513549\n      0.087593\n      0.188056\n      0.062423\n    \n  \n\n\n\n\n\ndf.plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ndf.plot.barh(stacked=True, alpha=0.5)\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\n5.1.3.3 plt.scatter()\n\nExample 5.9  \n\nimport numpy as np\n\nN = 100\ndata = 0.9 * np.random.rand(N, 2)\narea = (20 * np.random.rand(N))**2 \nc = np.sqrt(area)\nplt.scatter(data[:, 0], data[:, 1], s=area, marker='^', c=c)\n\n<matplotlib.collections.PathCollection at 0x10a4fbd3640>\n\n\n\n\n\n\n\n\n5.1.3.4 plt.hist()\nHere are two plots with build-in statistics. The plot command will have statistics as outputs. To disable it we could send the outputs to a temporary variable _. ::: {#exm-histogram1}\n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\ny = mu-30 + sigma*2 * np.random.randn(10000)\n_ = plt.hist(x, 50, density=True, facecolor='g', alpha=0.75)\n_ = plt.hist(y, 50, density=True, facecolor='r', alpha=0.75)\n\n\n\n\n:::\n\n\n\n5.1.4 plt.boxplot()\n\nExample 5.10  \n\nspread = np.random.rand(50) * 100\ncenter = np.ones(30) * 50\nflier_high = np.random.rand(10) * 100 + 100\nflier_low = np.random.rand(10) * -100\ndata = np.concatenate((spread, center, flier_high, flier_low)).reshape(50, 2)\n\n_ = plt.boxplot(data, flierprops={'markerfacecolor': 'g', 'marker': 'D'})\n\n\n\n\n\n\n\n5.1.5 Titles, labels and legends\n\nTitles\n\nplt.title(label), plt.xlabel(label), plt.ylabel(label) will set the title/xlabel/ylabel.\nax.set_title(label), ax.set_xlabel(label), ax.set_ylabel(label) will do the same thing.\n\nLabels\n\nplt methods\n\nxlim(), ylim(), xticks(), yticks(), xticklabels(), yticklabels()\nall the above with arguments\n\nax methods\n\nget_xlim(), get_ylim(), etc..\nset_xlim(), set_ylim(), etc..\n\n\nLegneds\n\nFirst add label option to each piece when plotting, and then add ax.legends() or plt.legends() at the end to display the legends.\nYou may use handles, labels = ax.get_legend_handles_labels() to get the handles and labels of the legends, and modify them if necessary.\n\n\n\nExample 5.11  \n\nimport numpy as np\nfig, ax = plt.subplots(1, 1)\nax.plot(np.random.randn(1000).cumsum(), 'k', label='one')\nax.plot(np.random.randn(1000).cumsum(), 'r--', label='two')\nax.plot(np.random.randn(1000).cumsum(), 'b.', label='three')\n\nax.set_title('Example')\nax.set_xlabel('x')\nax.set_ylabel('y')\n\nax.set_yticks([-40, 0, 40])\nax.set_yticklabels(['good', 'bad', 'ugly'])\n\nax.legend(loc='best')\n\n<matplotlib.legend.Legend at 0x10a50482a90>\n\n\n\n\n\n\n\n\n5.1.6 Annotations\n\nThe command to add simple annotations is ax.text(). The required auguments are the coordinates of the text and the text itself. You may add several options to modify the style.\nIf arrows are needed, we may use ax.annotation(). Here an arrow will be shown from xytext to xy. The style of the arrow is controlled by the option arrowprops.\n\n\nExample 5.12  \n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.plot(np.random.randn(1000).cumsum(), 'k', label='one')\nax.text(500, 0, 'Hello world!', family='monospace', fontsize=15, c='r')\nax.annotate('test', xy=(400, 0), xytext=(400, -10), c='r',\n            arrowprops={'facecolor': 'black',\n                        'shrink': 0.05})\n\nText(400, -10, 'test')\n\n\n\n\n\n\n\n\n5.1.7 Example\n\nExample 5.13 The stock data can be downloaded from here.\n\nfrom datetime import datetime\nfig, ax = plt.subplots()\ndata = pd.read_csv('assests/datasets/spx.csv', index_col=0, parse_dates=True)\nspx = data['SPX']\nspx.plot(ax=ax, style='k-')\ncrisis_data = [(datetime(2007, 10, 11), 'Peak of bull market'),\n               (datetime(2008, 3, 12), 'Bear Stearns Fails'),\n               (datetime(2008, 9, 15), 'Lehman Bankruptcy')]\nfor date, label in crisis_data:\n    ax.annotate(label, xy=(date, spx.asof(date) + 75),\n                xytext=(date, spx.asof(date) + 225),\n                arrowprops=dict(facecolor='black', headwidth=4, width=2,\n                                headlength=4),\n                horizontalalignment='left', verticalalignment='top')\nax.set_xlim(['1/1/2007', '1/1/2011'])\nax.set_ylim([600, 1800])\n_ = ax.set_title('Important dates in the 2008-2009 financial crisis')\n\n\n\n\n\n\nExample 5.14 Here is an example of arrows with different shapes. For more details please read the official document.\n\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 20, 1000)\nax.plot(x, np.cos(x))\nax.axis('equal')\n\nax.annotate('local maximum', xy=(6.28, 1), xytext=(10, 4),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nax.annotate('local minimum', xy=(5 * np.pi, -1), xytext=(2, -6),\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"angle3,angleA=0,angleB=-90\",\n                            color='r'))\n\nText(2, -6, 'local minimum')"
  },
  {
    "objectID": "contents/5/05-.html#seaborn",
    "href": "contents/5/05-.html#seaborn",
    "title": "5  Visualization",
    "section": "5.2 seaborn",
    "text": "5.2 seaborn\nThere are some new libraries built upon matplotlib, and seaborn is one of them. seaborn is for statistical graphics.\nseaborn is used imported in the following way.\n\nimport seaborn as sns\n\nseaborn also modifies the default matplotlib color schemes and plot styles to improve readability and aesthetics. Even if you do not use the seaborn API, you may prefer to import seaborn as a simple way to improve the visual aesthetics of general matplotlib plots.\nTo apply sns theme, run the following code.\n\nsns.set_theme()\n\nLet us directly run a few codes from the last section and compare the differences between them.\n\nExample 5.15  \n\nplt.plot(np.random.randn(30).cumsum(), color='r', linestyle='--', marker='o')\n\n\n\n\nPlease compare the output of the same code with the previous example\n\n\n5.2.1 Scatter plots with relplot()\nThe basic scatter plot method is scatterplot(). It is wrapped in relplot() as the default plotting method. So here we will mainly talk about relplot(). It is named that way because it is designed to visualize many different statistical relationships.\nThe idea of relplot() is to display points based on the variables x and y you choose, and assign different properties to alter the apperance of the points.\n\ncol will create multiple plots based on the column you choose.\nhue is for color encoding, based on the column you choose.\nsize will change the marker area, based on the column you choose.\nstyle will change the marker symbol, based on the column you choose.\n\n\nExample 5.16 Consider the following example. tips is a DataFrame, which is shown below.\n\nimport seaborn as sns\ntips = sns.load_dataset(\"tips\")\ntips\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      239\n      29.03\n      5.92\n      Male\n      No\n      Sat\n      Dinner\n      3\n    \n    \n      240\n      27.18\n      2.00\n      Female\n      Yes\n      Sat\n      Dinner\n      2\n    \n    \n      241\n      22.67\n      2.00\n      Male\n      Yes\n      Sat\n      Dinner\n      2\n    \n    \n      242\n      17.82\n      1.75\n      Male\n      No\n      Sat\n      Dinner\n      2\n    \n    \n      243\n      18.78\n      3.00\n      Female\n      No\n      Thur\n      Dinner\n      2\n    \n  \n\n244 rows × 7 columns\n\n\n\n\nsns.relplot(data=tips,\n            x=\"total_bill\", y=\"tip\", col=\"time\",\n            hue=\"smoker\", style=\"smoker\", size=\"size\")\n\n<seaborn.axisgrid.FacetGrid at 0x10a539cfd00>\n\n\n\n\n\n\nThe default type of plots for relplot() is scatter plots. However you may change it to line plot by setting kind='line'.\n\nExample 5.17  \n\ndots = sns.load_dataset(\"dots\")\nsns.relplot(data=dots, kind=\"line\",\n            x=\"time\", y=\"firing_rate\", col=\"align\",\n            hue=\"choice\", size=\"coherence\", style=\"choice\",\n            facet_kws=dict(sharex=False))\n\n<seaborn.axisgrid.FacetGrid at 0x10a53931e50>\n\n\n\n\n\n\n\n\n5.2.2 regplot()\nThis method is a combination between scatter plots and linear regression.\n\nExample 5.18 We still use tips as an example.\n\nsns.regplot(x='total_bill', y='tip', data=tips)\n\n<AxesSubplot:xlabel='total_bill', ylabel='tip'>\n\n\n\n\n\n\n\n\n5.2.3 pairplot()\nThis is a way to display the pairwise relations among several variables.\n\nExample 5.19 The following code shows the pairplots among all numeric data in tips.\n\nsns.pairplot(tips, diag_kind='kde', plot_kws={'alpha': 0.2})\n\n<seaborn.axisgrid.PairGrid at 0x10a53c58820>\n\n\n\n\n\n\n\n\n5.2.4 barplot\n\nExample 5.20  \n\nsns.barplot(x='total_bill', y='day', data=tips, orient='h')\n\n<AxesSubplot:xlabel='total_bill', ylabel='day'>\n\n\n\n\n\nIn the plot, there are several total_bill during each day. The value in the plot is the average of total_bill in each day, and the black line stands for the 95% confidence interval.\n\nsns.barplot(x='total_bill', y='day', hue='time', data=tips, orient='h')\n\n<AxesSubplot:xlabel='total_bill', ylabel='day'>\n\n\n\n\n\nIn this plot, lunch and dinner are distinguished by colors.\n\n\n\n5.2.5 Histogram\n\nExample 5.21  \n\nmu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\ny = mu-30 + sigma*2 * np.random.randn(10000)\ndf = pd.DataFrame(np.array([x,y]).T)\nsns.histplot(df, bins=100, kde=True)\n\n<AxesSubplot:ylabel='Count'>\n\n\n\n\n\nPlease compare this plot with this Example"
  },
  {
    "objectID": "contents/5/05-.html#examples",
    "href": "contents/5/05-.html#examples",
    "title": "5  Visualization",
    "section": "5.3 Examples",
    "text": "5.3 Examples\n\n5.3.1 Example 1: USA.gov Data From Bitly\nIn 2011, URL shortening service Bitly partnered with the US government website USA.gov to provide a feed of anonymous data gathered from users who shorten links ending with .gov or .mil. The data is gotten from [1].\nThe data file can be downloaded from here. The file is mostly in JSON. It can be converted into a DataFrame by the following code.\n\nimport pandas as pd\nimport numpy as np\nimport json\npath = 'assests/datasets/example.txt'\ndf = pd.DataFrame([json.loads(line) for line in open(path)])\n\nWe mainly use tz and a columns. So let us clean it.\n\ndf['tz'] = df['tz'].fillna('Missing')\ndf['tz'][df['tz'] == ''] = 'Unknown'\ndf['a'] = df['a'].fillna('Missing')\ndf['a'][df['a'] == ''] = 'Unknown'\n\nWe first want to extract the timezone infomation from it. The timezone info is in the column tz.\n\ntzone = df['tz']\ntvc = tzone.value_counts()\ntvc\n\nAmerica/New_York        1251\nUnknown                  521\nAmerica/Chicago          400\nAmerica/Los_Angeles      382\nAmerica/Denver           191\n                        ... \nEurope/Uzhgorod            1\nAustralia/Queensland       1\nEurope/Sofia               1\nAmerica/Costa_Rica         1\nAmerica/Tegucigalpa        1\nName: tz, Length: 98, dtype: int64\n\n\nAfter cleaning data, we would like to visulize the value counts.\n\nimport seaborn as sns\nsns.barplot(x=tvc[:10].values, y=tvc[:10].index)\n\n<AxesSubplot:>\n\n\n\n\n\nWe then would like to extract information from the column a. This column is about the agent of the connection. The important info is the part before the space ' '.\n\nagent = df['a']\nagent = agent.str.split(' ').str[0]\navc = agent.value_counts()\navc[:10]\n\nMozilla/5.0                 2594\nMozilla/4.0                  601\nGoogleMaps/RochesterNY       121\nMissing                      120\nOpera/9.80                    34\nTEST_INTERNET_AGENT           24\nGoogleProducer                21\nMozilla/6.0                    5\nBlackBerry8520/5.0.0.681       4\nBlackBerry8520/5.0.0.592       3\nName: a, dtype: int64\n\n\nNow let us assume that, if Windows appears in column a the user is using Windows os, if not then not. In this case, the os can be detected by the following code.\n\ndf['os'] = np.where(df['a'].str.contains('Windows'), 'Windows', 'Not Windows')\n\nNow we can make a bar plot about the counts based on os and timezone.\n\ntz_os_counts = df.groupby(['tz', 'os']).size().unstack().fillna(0)\ntz_os_counts.head()\n\n\n\n\n\n  \n    \n      os\n      Not Windows\n      Windows\n    \n    \n      tz\n      \n      \n    \n  \n  \n    \n      Africa/Cairo\n      0.0\n      3.0\n    \n    \n      Africa/Casablanca\n      0.0\n      1.0\n    \n    \n      Africa/Ceuta\n      0.0\n      2.0\n    \n    \n      Africa/Johannesburg\n      0.0\n      1.0\n    \n    \n      Africa/Lusaka\n      0.0\n      1.0\n    \n  \n\n\n\n\nWe then turn it into a DataFrame using the .stack(), .unstack() tricks.\n\ntovc = tz_os_counts.stack()[tz_os_counts.sum(axis=1).nlargest(10).index]\ntovc.name = 'count'\ndftovc = pd.DataFrame(tovc).reset_index()\n\nFinally we may draw the bar plot.\n\nsns.barplot(x='count', y='tz', hue='os', data=dftovc)\n\n<AxesSubplot:xlabel='count', ylabel='tz'>\n\n\n\n\n\n\n\n5.3.2 Example 2: US Baby Names 1880–2010\nThe United States Social Security Administration (SSA) has made available data on the frequency of baby names from 1880 through the present. Hadley Wickham, an author of several popular R packages, has often made use of this dataset in illustrating data manipulation in R. The dataset can be downloaded from here as a zip file. Please unzip it and put it in your working folder.\nIn the folder there are 131 .txt files. The naming scheme is yob + the year. Each file contains 3 columns: name, gender, and counts. We would like to add a column year, and combine all files into a single DataFrame. In our example, the year is from 1880 to 2010.\n\nimport pandas as pd\n\npath = 'assests/datasets/babynames/'\ndflist = list()\nfor year in range(1880, 2011):\n    filename = path + 'yob' + str(year) + '.txt'\n    df = pd.read_csv(filename, names=['name', 'gender', 'counts'])\n    df['year'] = year\n    dflist.append(df)\ndf = pd.concat(dflist, ignore_index=True)\n\nWe can plot the total births by sex and year.\n\nimport seaborn as sns\n\nsns.relplot(data=df.groupby(['gender', 'year']).sum().reset_index(),\n            x='year', y='counts', hue='gender', kind='line')\n\n\n\n<seaborn.axisgrid.FacetGrid at 0x10a58790dc0>\n\n\n\n\n\nFor further analysis, we would like to compute the proportions of each name relative to the total number of births per year per gender.\n\ndef add_prop(group):\n    group['prop'] = group.counts / group.counts.sum()\n    return group\n\ndf = df.groupby(['gender', 'year']).apply(add_prop)\ndf.head()\n\n\n\n\n\n\n\n  \n    \n      \n      name\n      gender\n      counts\n      year\n      prop\n    \n  \n  \n    \n      0\n      Mary\n      F\n      7065\n      1880\n      0.077643\n    \n    \n      1\n      Anna\n      F\n      2604\n      1880\n      0.028618\n    \n    \n      2\n      Emma\n      F\n      2003\n      1880\n      0.022013\n    \n    \n      3\n      Elizabeth\n      F\n      1939\n      1880\n      0.021309\n    \n    \n      4\n      Minnie\n      F\n      1746\n      1880\n      0.019188\n    \n  \n\n\n\n\nNow we would like to keep the first 100 names in each year, and save it as a new DataFrame top100.\n\ntop100 = (\n    df.groupby(['year', 'gender'])\n    .apply(lambda x: df.loc[x['counts'].nlargest(100).index])\n    .drop(columns=['year', 'gender'])\n    .reset_index()\n    .drop(columns='level_2')\n)\ntop100.head()\n\n\n\n\n\n\n\n  \n    \n      \n      year\n      gender\n      name\n      counts\n      prop\n    \n  \n  \n    \n      0\n      1880\n      F\n      Mary\n      7065\n      0.077643\n    \n    \n      1\n      1880\n      F\n      Anna\n      2604\n      0.028618\n    \n    \n      2\n      1880\n      F\n      Emma\n      2003\n      0.022013\n    \n    \n      3\n      1880\n      F\n      Elizabeth\n      1939\n      0.021309\n    \n    \n      4\n      1880\n      F\n      Minnie\n      1746\n      0.019188\n    \n  \n\n\n\n\nNote that level_2 is related to the original index after reset_index(). That’s why we don’t need it here.\nNow we would like to draw the trend of some names.\n\nnamelist = ['John', 'Harry', 'Mary']\nsns.relplot(data=top100[top100['name'].isin(namelist)],\n            x='year', y='counts', hue='name', kind='line')\n\n\n\n<seaborn.axisgrid.FacetGrid at 0x10a587905e0>\n\n\n\n\n\nNow we would like to analyze the ending of names.\n\ndf['ending'] = df['name'].str[-1]\nendingcount = df.groupby(['gender', 'year', 'ending']).sum().reset_index()\n\nWe would like to draw barplots to show the distributions in year 1910, 1960 and 2010.\n\ncertainyear = endingcount[endingcount['year'].isin([1910, 1960, 2010])]\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(2, 1, figsize=(10,7))\nsns.barplot(data=certainyear[endingcount['gender']=='M'],\n            x='ending', y='prop', hue='year', ax=axs[0])\nsns.barplot(data=certainyear[endingcount['gender']=='F'],\n            x='ending', y='prop', hue='year', ax=axs[1]).legend_.remove()\n\n\n\n\n\n\nWe would also like to draw the line plot to show the trending of certain letters through years.\n\nsns.relplot(data=endingcount[endingcount.ending.isin(['d', 'n', 'y'])],\n            x='year', y='prop', hue='ending', kind='line')\n\n\n\n<seaborn.axisgrid.FacetGrid at 0x10a59be0df0>"
  },
  {
    "objectID": "contents/5/05-.html#exercises",
    "href": "contents/5/05-.html#exercises",
    "title": "5  Visualization",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\nExercise 5.1 Please download the mtcars file from here and read it as a DataFrame. Then create a scatter plot of the drat and wt variables from mtcars and color the dots by the carb variable.\n\n\nExercise 5.2 Please consider the baby name dataset. Please draw the trends of counts of names ending in a, e, n across years for each gender."
  },
  {
    "objectID": "contents/5/05-.html#projects",
    "href": "contents/5/05-.html#projects",
    "title": "5  Visualization",
    "section": "5.5 Projects",
    "text": "5.5 Projects\n\nExercise 5.3 Please read the file as a DataFrame from here. This is the Dining satisfaction with quick service restaurants questionare data provided by Dr. Siri McDowall, supported by DART SEED grant.\n\nPlease pick out all rating columns. Excluding last.visit, visit.again and recommend, compute the mean of the rest and add it to the DataFrame as a new column.\nUse a plot to show the relations among these four columns: last.visit, visit.again, recommend and mean.\nLook at the column Profession. Keep Student, and change everything else to be Professional, and add it as a new column Status to the DataFrame.\nDraw the histogram of mean with respect to Status.\nFind the counts of each recommend rating for each Status and draw the barplot. Do the same to last.visit/Status and visit.again/Status.\nExploer the dataset and draw one plot.\n\n\n\nExercise 5.4 Please use the baby name dataset. We would like to consider the diversity of the names. Please compute the number of popular names in top 50% for each year each gender. Draw a line plot to show the trend and discuss the result.\n\n\n\n\n\n[1] McKinney, W. (2017). Python for data analysis: Data wrangling with pandas, NumPy, and IPython. O’Reilly Media."
  },
  {
    "objectID": "contents/8/08-.html#another-example",
    "href": "contents/8/08-.html#another-example",
    "title": "8  R for Data Sciences",
    "section": "8.6 Another example",
    "text": "8.6 Another example\nLet us use R to solve the babynames dataset again.\nThe first task is to read those files.\n\nExample 8.12 Please read files and put the data into one tibble. The dataset can be downloaded from here as a zip file.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\npath <- 'assessts/datasets/babynames/yob'\ndfs <- lapply(1880:2010, function(y){\n    filepath <- paste0(path, as.character(y), '.txt')\n    df_individual <- tibble(read.csv(filepath, header=FALSE))\n    names(df_individual) <- c('name', 'gender', 'counts')\n    df_individual$year <- y\n    df_individual\n})\ndf <- bind_rows(dfs)\n\n\n\n\n\n\nExample 8.13 Please plot the total births by gender and year.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %>% \n    group_by(gender, year) %>% \n    summarize(total_num=sum(counts)) %>% \n    ggplot() +\n        geom_line(mapping = aes(x=year, y=total_num, color=gender))\n#> `summarise()` has grouped output by 'gender'. You can override using the\n#> `.groups` argument.\n\n\n\n\n\n\n\n\n\nExample 8.14 Please compute the proportions of each name relateive to the total number of births per year per gender.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %>% \n    group_by(gender, year) %>% \n    mutate(prop=counts/sum(counts))\n#> # A tibble: 1,690,784 × 5\n#> # Groups:   gender, year [262]\n#>    name      gender counts  year   prop\n#>    <chr>     <chr>   <int> <int>  <dbl>\n#>  1 Mary      F        7065  1880 0.0776\n#>  2 Anna      F        2604  1880 0.0286\n#>  3 Emma      F        2003  1880 0.0220\n#>  4 Elizabeth F        1939  1880 0.0213\n#>  5 Minnie    F        1746  1880 0.0192\n#>  6 Margaret  F        1578  1880 0.0173\n#>  7 Ida       F        1472  1880 0.0162\n#>  8 Alice     F        1414  1880 0.0155\n#>  9 Bertha    F        1320  1880 0.0145\n#> 10 Sarah     F        1288  1880 0.0142\n#> # … with 1,690,774 more rows\n\n\n\n\n\n\nExample 8.15 We would like to keep the first 100 names (by counts) in each year and save it as a new tibble top100.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntop100 <- df %>% \n        group_by(gender, year) %>% \n        top_n(100, wt=counts)\n\n\n\n\n\n\nExample 8.16 Please draw the trend of John, Harry, Mary in top100 by counts.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nnamelist <- c('John', 'Harry', 'Mary')\ntop100 %>% \n    filter(name %in% namelist) %>% \n    ggplot() +\n        geom_line(mapping=aes(x=year, y=counts, color=name))\n\n\n\n\n\n\n\n\n\nExample 8.17 Now we would like to analyze the ending of names. Please get a tibble that contains the counts of ending letter per year per gender. We mainly focus on 1910, 1960 and 2010.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %>% \n    filter(year %in% c(1910, 1960, 2010)) %>% \n    mutate(ending=str_sub(name, -1, -1)) %>% \n    group_by(gender, year, ending) %>% \n    summarise(ending_counts=sum(counts)) %>% \n    ggplot() +\n        geom_bar(\n            mapping = aes(\n                x=ending, \n                y=ending_counts, \n                fill=year,\n                group=year\n                ), \n            stat = \"identity\", \n            position = \"dodge\",\n        ) +\n        facet_wrap(~gender, nrow=2)\n\n\n\n\n\n\n\n\n\nExample 8.18 Please draw the line plot to show the trending of certain letters through years. Here we choose d, n and y.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %>% \n    mutate(ending=str_sub(name, -1, -1)) %>% \n    group_by(year, ending) %>% \n    summarise(ending_counts=sum(counts)) %>% \n    filter(ending %in% c('d', 'n', 'y')) %>% \n    ggplot() +\n        geom_line(\n            mapping = aes(\n                x=year, \n                y=ending_counts, \n                color=ending\n            )\n        )"
  },
  {
    "objectID": "contents/8/08-.html#examples",
    "href": "contents/8/08-.html#examples",
    "title": "8  R for Data Sciences",
    "section": "8.5 Examples",
    "text": "8.5 Examples\n\n8.5.1 WHO TB dataset\nLet us explore the tuberculosis cases data. The dataset is provided by WHO and can be downloaded from here. tidyr also provides the dataset. You may directly get the dataset after you load tidyr from tidyverse. The variable description can be found from tidyr documentations.\n\nlibrary(tidyverse)\nwho\n#> # A tibble: 7,240 × 60\n#>    country  iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544\n#>    <chr>    <chr> <chr> <int>       <int>        <int>        <int>        <int>\n#>  1 Afghani… AF    AFG    1980          NA           NA           NA           NA\n#>  2 Afghani… AF    AFG    1981          NA           NA           NA           NA\n#>  3 Afghani… AF    AFG    1982          NA           NA           NA           NA\n#>  4 Afghani… AF    AFG    1983          NA           NA           NA           NA\n#>  5 Afghani… AF    AFG    1984          NA           NA           NA           NA\n#>  6 Afghani… AF    AFG    1985          NA           NA           NA           NA\n#>  7 Afghani… AF    AFG    1986          NA           NA           NA           NA\n#>  8 Afghani… AF    AFG    1987          NA           NA           NA           NA\n#>  9 Afghani… AF    AFG    1988          NA           NA           NA           NA\n#> 10 Afghani… AF    AFG    1989          NA           NA           NA           NA\n#> # … with 7,230 more rows, and 52 more variables: new_sp_m4554 <int>,\n#> #   new_sp_m5564 <int>, new_sp_m65 <int>, new_sp_f014 <int>,\n#> #   new_sp_f1524 <int>, new_sp_f2534 <int>, new_sp_f3544 <int>,\n#> #   new_sp_f4554 <int>, new_sp_f5564 <int>, new_sp_f65 <int>,\n#> #   new_sn_m014 <int>, new_sn_m1524 <int>, new_sn_m2534 <int>,\n#> #   new_sn_m3544 <int>, new_sn_m4554 <int>, new_sn_m5564 <int>,\n#> #   new_sn_m65 <int>, new_sn_f014 <int>, new_sn_f1524 <int>, …\n\nBased on the description of varaibles, we understand that\n\ncountry, iso2, iso3 are all refered to country names (and thus they are redundant).\nColumns after year, like new_sp_m014 etc., are counts of new TB cases recorded by groups. The code has three parts, most of which are separated by _ (but there are some exceptions).\n\nThe first part is always new.\nThe second part is a code for method of diagnosis:\n\nrel = relapse,\nsn = negative pulmonary smear,\nsp = positive pulmonary smear,\nep = extrapulmonary.\n\nThe third part is a code for gender (f = female, m = male) and a code for age group:\n\n014 = 0-14 yrs of age,\n1524 = 15-24 years of age,\n2534 = 25 to 34 years of age,\n3544 = 35 to 44 years of age,\n4554 = 45 to 54 years of age,\n5564 = 55 to 64 years of age,\n65 = 65 years of age or older\n\n\n\nTherefore to clean the data, we need the following steps.\n\nExample 8.9 Gather together all the columns from new_sp_m014 to newrel_f65.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nwholonger <- who %>% pivot_longer(cols=5:60, names_to='group', values_to='counts')\n\n\n\n\n\nThen we use stringr::str_replace() to replace newrel by new_rel.\n\nwholonger2 <- wholonger %>% mutate(key=str_replace(group, 'newrel', 'new_rel'))\n\n\nExample 8.10 Parse the column group into columns.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nwholonger3 <- wholonger2 %>% \n        separate(key, into=c('new', 'type', 'genderage'), sep='_') %>% \n        separate(genderage, into=c('gender', 'age'), sep=1)\n\n\n\n\n\n\nExample 8.11 Pick the columns that matters.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntidywho <- wholonger3[c('country', 'year', 'type', 'gender', 'age', 'counts')]\n\n\n\n\n\nWe could use the pipe symbol to connect all the above steps.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntidywho <- who %>% \n    pivot_longer(cols=5:60, names_to='group', values_to='counts') %>% \n    mutate(key=str_replace(group, 'newrel', 'new_rel')) %>% \n    separate(key, into=c('new', 'type', 'genderage'), sep='_') %>% \n    separate(genderage, into=c('gender', 'age'), sep=1) %>% \n    select('country', 'year', 'type', 'gender', 'age', 'counts')\n\n\n\n\n\n\n8.5.2 US Babynames\nLet us use R to solve the babynames dataset again.\nThe first task is to read those files.\n\nExample 8.12 Please read files and put the data into one tibble. The dataset can be downloaded from here as a zip file.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\npath <- 'assessts/datasets/babynames/yob'\ndfs <- map(1880:2010, function(y){\n    filepath <- paste0(path, as.character(y), '.txt')\n    df_individual <- tibble(read.csv(filepath, header=FALSE))\n    names(df_individual) <- c('name', 'gender', 'counts')\n    df_individual$year <- y\n    df_individual\n})\ndf <- bind_rows(dfs)\n\n\n\n\n\n\nExample 8.13 Please plot the total births by gender and year.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %>% \n    group_by(gender, year) %>% \n    summarize(total_num=sum(counts)) %>% \n    ggplot() +\n        geom_line(mapping = aes(x=year, y=total_num, color=gender))\n#> `summarise()` has grouped output by 'gender'. You can override using the\n#> `.groups` argument.\n\n\n\n\n\n\n\n\n\nExample 8.14 Please compute the proportions of each name relateive to the total number of births per year per gender.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %>% \n    group_by(gender, year) %>% \n    mutate(prop=counts/sum(counts))\n#> # A tibble: 1,690,784 × 5\n#> # Groups:   gender, year [262]\n#>    name      gender counts  year   prop\n#>    <chr>     <chr>   <int> <int>  <dbl>\n#>  1 Mary      F        7065  1880 0.0776\n#>  2 Anna      F        2604  1880 0.0286\n#>  3 Emma      F        2003  1880 0.0220\n#>  4 Elizabeth F        1939  1880 0.0213\n#>  5 Minnie    F        1746  1880 0.0192\n#>  6 Margaret  F        1578  1880 0.0173\n#>  7 Ida       F        1472  1880 0.0162\n#>  8 Alice     F        1414  1880 0.0155\n#>  9 Bertha    F        1320  1880 0.0145\n#> 10 Sarah     F        1288  1880 0.0142\n#> # … with 1,690,774 more rows\n\n\n\n\n\n\nExample 8.15 We would like to keep the first 100 names (by counts) in each year and save it as a new tibble top100.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ntop100 <- df %>% \n        group_by(gender, year) %>% \n        top_n(100, wt=counts)\n\n\n\n\n\n\nExample 8.16 Please draw the trend of John, Harry, Mary in top100 by counts.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nnamelist <- c('John', 'Harry', 'Mary')\ntop100 %>% \n    filter(name %in% namelist) %>% \n    ggplot() +\n        geom_line(mapping=aes(x=year, y=counts, color=name))\n\n\n\n\n\n\n\n\n\nExample 8.17 Now we would like to analyze the ending of names. Please get a tibble that contains the counts of ending letter per year per gender. We mainly focus on 1910, 1960 and 2010.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %>% \n    filter(year %in% c(1910, 1960, 2010)) %>% \n    mutate(ending=str_sub(name, -1, -1)) %>% \n    group_by(gender, year, ending) %>% \n    summarise(ending_counts=sum(counts)) %>% \n    ggplot() +\n        geom_bar(\n            mapping = aes(\n                x=ending, \n                y=ending_counts, \n                fill=year,\n                group=year\n                ), \n            stat = \"identity\", \n            position = \"dodge\",\n        ) +\n        facet_wrap(~gender, nrow=2)\n\n\n\n\n\n\n\n\n\nExample 8.18 Please draw the line plot to show the trending of certain letters through years. Here we choose d, n and y.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndf %>% \n    mutate(ending=str_sub(name, -1, -1)) %>% \n    group_by(year, ending) %>% \n    summarise(ending_counts=sum(counts)) %>% \n    filter(ending %in% c('d', 'n', 'y')) %>% \n    ggplot() +\n        geom_line(\n            mapping = aes(\n                x=year, \n                y=ending_counts, \n                color=ending\n            )\n        )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python/R for Data Science",
    "section": "",
    "text": "Preface\nThis is the lecture notes for STAT 2304 Programming languages for Data Science 2022 Fall at ATU. If you have any comments/suggetions/concers about the notes please contact me at my email xxiao@atu.edu."
  },
  {
    "objectID": "contents/references.html",
    "href": "contents/references.html",
    "title": "References",
    "section": "",
    "text": "[1] Klosterman, S.\n(2021). Data\nscience projects with python: A case study approach to gaining valuable\ninsights from real data with machine learning. Packt\nPublishing, Limited.\n\n\n[2] McKinney, W.\n(2017). Python for data analysis: Data wrangling with pandas, NumPy,\nand IPython. O’Reilly Media.\n\n\n[3] Shaw, Z. A.\n(2017). Learn\npython 3 the hard way. Addison Wesley.\n\n\n[4] Sweigart, A.\n(2020). Automate the\nboring stuff with python, 2nd edition practical programming for total\nbeginners: Practical programming for total beginners. No Starch\nPress.\n\n\n[5] Prabhakaran, S.\n(2018). 101\nNumPy exercises for data analysis (python).\n\n\n[6] Grolemund, G.\n(2014). Hands-on programming with r: Write your own functions and\nsimulations. O’Reilly Media.\n\n\n[7] Prabhakaran, S.\n(2018). 101\npandas exercises for data analysis.\n\n\n[8] Beuzen, T. and\nTimbers, T. (2022). Python\npackages. Taylor & Francis Group.\n\n\n[9] Wickham, H. and\nGrolemund, G. (2017). R for data science: Import, tidy,\ntransform, visualize, and model data. O’Reilly Media.\n\n\n[10] Youens-Clark, K.\n(2020). Tiny python\nprojects. Manning Publications."
  }
]